{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4959385c-b2cf-4297-a247-2df520100f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jbloom/Projects/AstroML/data\n"
     ]
    }
   ],
   "source": [
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d098fd5-d62e-4a2b-b076-58bf6fe5517d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "sns version: 0.13.0\n",
      "astroML version: 1.0.2\n",
      "torch version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import logging\n",
    "import joblib\n",
    "import zipfile\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "logging.captureWarnings(True)\n",
    "from scipy.signal import medfilt\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from astropy.wcs import WCS\n",
    "from astropy.io import fits\n",
    "from astroML.time_series import search_frequencies, lomb_scargle, MultiTermFit\n",
    "\n",
    "from astropy import units as u\n",
    "import astroML\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "\n",
    "from preprocess_data import clip_outliers\n",
    "print(f\"sns version: {sns.__version__}\")\n",
    "print(f\"astroML version: {astroML.__version__}\")\n",
    "print(f\"torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedcc55e-0fcd-4fea-ba1f-c7b704c9f4b2",
   "metadata": {},
   "source": [
    "# ASASSN Data\n",
    "\n",
    "## Preparation:\n",
    "\n",
    "   - Create a folder to hold the data (`data/asaasn`) \n",
    "   - Download the V-band data (https://drive.google.com/drive/folders/1IAtztpddDeh5XOiuxmLWdLUaT_quXkug)\n",
    "     Make sure you have the files in the data directory `asassn_catalog_full.csv` and `asassnvarlc_vband_complete.zip`\n",
    "     (Do not unzip the light curve file!)\n",
    "   - Download the g-band data (https://drive.google.com/drive/folders/1gxcIokRsw1eyPmbPZ0-C8blfRGItSOAu)\n",
    "     Make sure you have the files `asassn_variables_x.csv` and `g_band_lcs-001.tar.gz`\n",
    "     (Do not unzip the light curve file!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fc96bd-ee87-4a38-beae-04585a820974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9948fbb6-44eb-47cb-9a1e-4dfcd6010e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "asassn_dir = Path(\"data/asaasn\")\n",
    "raw_data_files={\"v\": {\"tab\": \"asassn_catalog_full.csv\", \"lcs\": \"asassnvarlc_vband_complete.zip\", \"prefix\": \"vardb_files/\", \n",
    "                      \"filekey\": \"asassn_name\", \"keyfill\": \"\", \"flux_headers\": [\"HJD\", \"FLUX\", \"FLUX_ERR\"], \"mag_headers\": [\"HJD\",\"MAG\",\"MAG_ERR\"]},\n",
    "                \"g\": {\"tab\": \"asassn_variables_x.csv\", \"lcs\": \"g_band_lcs-001.tar\", \"prefix\": \"g_band_lcs/\", \n",
    "                      \"filekey\": \"ID\", \"keyfill\": \"_\", \"flux_headers\": [\"HJD\",\"flux\",\"flux_err\"],\"mag_headers\": [\"HJD\",\"mag\",\"mag_err\"]}}\n",
    "metadata_cols={\"g\": ['Mean_gmag', 'Amplitude', 'Period', 'parallax', 'parallax_error', 'parallax_over_error', 'pm', 'pmra', 'pmra_error', 'pmdec', 'pmdec_error', 'ruwe', 'phot_g_mean_mag', 'e_phot_g_mean_mag', 'phot_bp_mean_mag', 'e_phot_bp_mean_mag', 'phot_rp_mean_mag', 'e_phot_rp_mean_mag', 'bp_rp', 'FUVmag', 'e_FUVmag', 'NUVmag', 'e_NUVmag', 'W1mag', 'W2mag', 'W3mag', 'W4mag', 'Jmag', 'Hmag', 'Kmag', 'e_W1mag', 'e_W2mag', 'e_W3mag', 'e_W4mag', 'e_Jmag', 'e_Hmag', 'e_Kmag'],\n",
    "                 \"v\": ['mean_vmag', 'amplitude', 'period', 'phot_g_mean_mag', 'e_phot_g_mean_mag', \"lksl_statistic\",\"rfr_score\", 'phot_bp_mean_mag', 'e_phot_bp_mean_mag', 'phot_rp_mean_mag', 'e_phot_rp_mean_mag', 'bp_rp', 'parallax', 'parallax_error', 'parallax_over_error', 'pmra', 'pmra_error', 'pmdec', 'pmdec_error', 'j_mag', 'e_j_mag', 'h_mag', 'e_h_mag', 'k_mag', 'e_k_mag', 'w1_mag', 'e_w1_mag', 'w2_mag', 'e_w2_mag', 'w3_mag', 'e_w3_mag', 'w4_mag', 'e_w4_mag', 'j_k', 'w1_w2', 'w3_w4', 'apass_vmag', 'e_apass_vmag', 'apass_bmag', 'e_apass_bmag', 'apass_gpmag', \n",
    "                       'e_apass_gpmag', 'apass_rpmag', 'e_apass_rpmag', 'apass_ipmag', 'e_apass_ipmag', 'FUVmag', 'e_FUVmag', 'NUVmag', 'e_NUVmag', 'pm', 'ruwe']              }\n",
    "bookkeeping_cols={\"v\":  ['id', 'source_id', 'asassn_name', 'other_names', 'raj2000', 'dej2000', 'l', 'b', 'epoch_hjd', 'gdr2_id', 'allwise_id', 'apass_dr9_id','edr3_source_id', 'galex_id','tic_id'],\n",
    "                    \"g\":  ['ID', 'RAJ2000', 'DEJ2000', 'l', 'b', 'EpochHJD', 'EDR3_source_id', 'GALEX_ID',  'TIC_ID', 'AllWISE_ID', \"ML_probability\",'class_probability']}\n",
    "target_cols={\"g\":  ['ML_classification'],\n",
    "             \"v\":  ['variable_type']}\n",
    "period_col = {\"g\": 'Period', \"v\": \"period\"}\n",
    "merge_key={\"g\": \"EDR3_source_id\", \"v\": \"edr3_source_id\"}\n",
    "\n",
    "\n",
    "class ASASSNVarStarDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_root, prediction_length, mode='train', use_errors=True,\n",
    "                 use_bands=[\"v\",\"g\"],merge_type=\"inner\",lc_type=\"flux\",\n",
    "                 window_length=None, \n",
    "                 rng=None, return_phased = True, lock_phase=None, clean=True, recalc_period=False, \n",
    "                 verbose=False, lamost_spec_file=\"Spectra/lamost_spec.csv\",\n",
    "                 lamost_spec_dir=\"Spectra/v2\",only_sources_with_spectra=True, prime=True,\n",
    "                 initial_clean_clip=[20,5],only_periodic=True, period_cache = \"periods.csv\",\n",
    "                 return_items_as_list=True\n",
    "                ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Multi-modal ASASSN dataset of variable stars\n",
    "        \n",
    "        rng = random state. If rng is None then set the state\n",
    "        return_phased = return the phased light curve as well as the original light curves\n",
    "        recalc_period = refit the L-S to find the best period and rewrites logP?\n",
    "        return_ls = refit the L-S \n",
    "        cache_ls_dir = cache dir of the L-S\n",
    "        meta_data_columns = columns to return as metadata\n",
    "        \"\"\"\n",
    "        self.data_root = data_root\n",
    "        self.prediction_length = prediction_length\n",
    "        self.use_errors = use_errors\n",
    "        self.window_length = window_length\n",
    "        self.return_phased = return_phased\n",
    "        self.recalc_period = recalc_period\n",
    "        self.clean=clean\n",
    "        self.verbose = verbose\n",
    "        self.use_bands = use_bands\n",
    "        if not isinstance(use_bands, list):\n",
    "            raise Exception(\"`use_bands` must be a list like ['v', 'g']\")\n",
    "        self.merge_type = merge_type\n",
    "        self.lc_type = lc_type\n",
    "        self.lamost_spec_file = lamost_spec_file\n",
    "        self.lamost_spec_dir = lamost_spec_dir\n",
    "        self.only_sources_with_spectra = only_sources_with_spectra\n",
    "        self.lock_phase = lock_phase\n",
    "        self.initial_clean_clip = initial_clean_clip\n",
    "        self.only_periodic = only_periodic\n",
    "        self.period_cache = period_cache\n",
    "        self.return_items_as_list = return_items_as_list\n",
    "        \n",
    "        # set the random seed if need be\n",
    "        if rng is None:\n",
    "            self.rng = np.random.default_rng(42)\n",
    "        else:\n",
    "            self.rng = rng\n",
    "\n",
    "        self._check_and_open_data_files()\n",
    "        self._merge_bands()\n",
    "        \n",
    "        if self.recalc_period and self.period_cache is not None:\n",
    "            fname = self.data_root / self.period_cache\n",
    "            try:\n",
    "                self.period_recalc_df = pd.read_csv(fname)\n",
    "                if self.verbose:\n",
    "                    print(\"Opened period cache file\")\n",
    "            except:\n",
    "                self.period_recalc_df  = pd.DataFrame(columns=[\"id\",\"p\", \"band\"])\n",
    "\n",
    "        if prime:\n",
    "            self._prime()\n",
    "\n",
    "        # shuffle \n",
    "        self.df = self.df.sample(frac=1, random_state=self.rng)\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.recalc_period and self.period_cache is not None:\n",
    "            # read in what's on disk just in case it changed by another process.\n",
    "            if (self.data_root / self.period_cache).exists():\n",
    "                tmp = pd.read_csv(self.data_root / self.period_cache)\n",
    "                merged = pd.concat([tmp,self.period_recalc_df], ignore_index=True).drop_duplicates(keep=\"last\",ignore_index=True)\n",
    "            else:\n",
    "                merged =  self.period_recalc_df\n",
    "            merged.to_csv(self.data_root / self.period_cache, index=False)\n",
    "            if self.verbose:\n",
    "                print(\"Wrote period cache file.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if not isinstance(idx, list):\n",
    "            idx = [idx]\n",
    "\n",
    "        sources = self.df.iloc[idx]\n",
    "\n",
    "        return_dict = {}\n",
    "        \n",
    "        # bookkeeping - not to be used in learning\n",
    "        return_dict[\"bookkeeping_data\"] = sources[self.bookkeeping_all].values.tolist()\n",
    "        \n",
    "        # classes\n",
    "        targets = sources[self.target_all].values\n",
    "        for k, v in self.target_lookup.items(): targets[targets==v] = k\n",
    "        \n",
    "        return_dict[\"classes\"] = targets.astype(np.int32)\n",
    "        \n",
    "        # light curves\n",
    "        return_dict[\"lcs\"] = self.get_light_curves(sources)\n",
    "\n",
    "        # spectra\n",
    "        return_dict[\"spectra\"] = self.get_spectra(sources)\n",
    "\n",
    "        if self.return_phased:\n",
    "            phased_all = []\n",
    "            i = 0\n",
    "            Ps = {band: [] for band in self.use_bands} \n",
    "            for ind, source in sources.iterrows():\n",
    "                source_id = source[merge_key[self.use_bands[0]]]\n",
    "                band_periods = {band: source[period_col[band]] for band in self.use_bands}\n",
    "                used_cache = {band: False for band in self.use_bands}\n",
    "                for band_num, band in enumerate(self.use_bands):\n",
    "                    recalc = self.recalc_period\n",
    "                    if len(self.use_bands) > 1:\n",
    "                        other_band = list(filter(lambda x: x != band, self.use_bands.copy()))[0]\n",
    "                    else:\n",
    "                        other_band = band\n",
    "                    if self.lock_phase is not None:\n",
    "                        if band == self.lock_phase:\n",
    "                            if np.isnan(band_periods[band]):\n",
    "                                # switch to the other band\n",
    "                                band_periods[band] = band_periods[other_band]\n",
    "                        else:\n",
    "                            band_periods[band] = band_periods[other_band]\n",
    "                            used_cache[band] = True\n",
    "                            continue\n",
    "                            \n",
    "                    # failsafe: if the period is nan then recalc\n",
    "                    if np.isnan(band_periods[band]):\n",
    "                        recalc = True\n",
    "\n",
    "                    if recalc:\n",
    "                        # print(f\"{band} P={band_periods[band]} \",flush=True, end=\"\")\n",
    "                        lcs = return_dict[\"lcs\"][i][band_num]\n",
    "                        t, y, dy = lcs[:,0], lcs[:,1], lcs[:,2]\n",
    "                        do_calc = True\n",
    "                        if self.period_cache is not None and self.recalc_period:\n",
    "                            tmp = self.period_recalc_df.query(f\"(id == '{source_id}') & (band == '{band}')\")[\"p\"].values\n",
    "                            if len(tmp) > 0:\n",
    "                                P_best = tmp[-1]\n",
    "                                do_calc = False\n",
    "                                used_cache[band] = True\n",
    "                                if self.verbose:\n",
    "                                    print(\" got period from cache \")\n",
    "                            else:\n",
    "                                do_calc = True\n",
    "                        if do_calc and len(t) > 20:\n",
    "                            t, y, dy, _, P_best, _, _, _ = \\\n",
    "                                clip_outliers(t, y, dy, measurements_in_flux_units = self.lc_type == \"flux\", \n",
    "                                          initial_clip=self.initial_clean_clip, clean_only=False, max_iter=2)\n",
    "                        # print(f\" ({P_best}) \", flush=True, end=\"\")\n",
    "                        # we may have found a harmonic. Fix it.\n",
    "                        if abs((P_best - band_periods[band])/band_periods[band]) > 0.01 and P_best < band_periods[band]:\n",
    "                            P_best *= 2\n",
    "                        elif abs(P_best - 1) < 0.01 or abs(P_best - 2) < 0.01 or abs(P_best - 3) < 0.01:\n",
    "                            # keep the original P if we're too close to 1 or 2 or 3 day Periods\n",
    "                            P_best = band_periods[band]\n",
    "                            if np.isnan(P_best):\n",
    "                                P_best = band_periods[other_band]\n",
    "\n",
    "                        band_periods[band] = P_best\n",
    "                        # print(f\"P={band_periods[band]} \",flush=True)\n",
    "\n",
    "                # now that we have the period, fold\n",
    "                phased = []\n",
    "                for band_num, band in enumerate(self.use_bands):\n",
    "                    if len(self.use_bands) > 1:\n",
    "                        other_band = list(filter(lambda x: x != band, self.use_bands.copy()))[0]\n",
    "                    else:\n",
    "                        other_band = band\n",
    "                    P_best = band_periods[band] if band == self.lock_phase else band_periods[other_band]   \n",
    "                    lcs = return_dict[\"lcs\"][i][band_num]\n",
    "                    t, y, dy = lcs[:,0], lcs[:,1], lcs[:,2]\n",
    "                    phased.append(np.vstack(( (t % P_best)/P_best, y, dy)).T)\n",
    "                    Ps[band].append(P_best)\n",
    "                    if self.period_cache is not None and self.recalc_period and not used_cache[band]:\n",
    "                        # append the recalc periods to the cache\n",
    "                        self.period_recalc_df.loc[len(self.period_recalc_df)] = [source_id, P_best, band]\n",
    "                            \n",
    "                phased_all.append(phased)\n",
    "                i+=1\n",
    "                \n",
    "            # update the sources table with the new periods\n",
    "            sources[period_col[band]] = Ps[band]\n",
    "            return_dict[\"phased\"] = phased_all\n",
    "\n",
    "        # metadata\n",
    "        return_dict[\"metadata\"] = sources[self.metadata_all].values\n",
    "\n",
    "        if not self.return_items_as_list:\n",
    "            return return_dict\n",
    "        else:\n",
    "            if self.return_phased:\n",
    "                lcs = return_dict[\"phased\"]\n",
    "            else:\n",
    "                lcs = return_dict[\"lcs\"]\n",
    "            return lcs, return_dict[\"metadata\"], return_dict[\"spectra\"], return_dict[\"classes\"], return_dict[\"bookkeeping_data\"]\n",
    "            \n",
    "    def _prime(self):\n",
    "        \"\"\" This takes about 1 minute. After that getting light curves is fast \"\"\"\n",
    "        print(\"Priming tarballs by doing initial scan...\", flush=True, end=\"\")\n",
    "        self.get_light_curves(self.df.sample(random_state=self.rng))\n",
    "        print(\"done.\", flush=True)\n",
    "        \n",
    "    def get_light_curves(self, rows):\n",
    "        \"\"\" Given df row(s), return the light curves as numpy arrays \"\"\"\n",
    "        \n",
    "        light_curves = []\n",
    "        for ind, row in rows.iterrows():\n",
    "            row_lc = []\n",
    "            for band in self.use_bands:\n",
    "                name = row[raw_data_files[band][\"filekey\"]].replace(\" \", raw_data_files[band][\"keyfill\"])\n",
    "                if self.lcs[band][1] == \"tar\" and band == \"g\":\n",
    "                    try:\n",
    "                        f = self.lcs[band][0].getmember(f\"{raw_data_files[band]['prefix']}{name}.dat\")\n",
    "                    except KeyError:\n",
    "                        print(f\"Cannot find {raw_data_files[band]['prefix']}{name}\")\n",
    "                        continue\n",
    "                    row_lc.append(pd.read_csv(self.lcs[band][0].extractfile(f), sep=\"\\t\")[raw_data_files[band][f\"{self.lc_type}_headers\"]].values)\n",
    "                elif self.lcs[band][1] == \"zip\" and band == \"v\":\n",
    "                    try:\n",
    "                        f = self.lcs[band][0].open(f\"{raw_data_files[band]['prefix']}{name}.dat\")\n",
    "                    except KeyError:\n",
    "                        print(f\"Cannot find {raw_data_files[band]['prefix']}{name}\")\n",
    "                        continue\n",
    "                    row_lc.append(pd.read_csv(f, sep=\" \", skiprows=1)[raw_data_files[band][f\"{self.lc_type}_headers\"]].values)                \n",
    "                else:\n",
    "                    raise Exception(\"Dont know how to get data from such files\")\n",
    "\n",
    "                if self.clean and len(row_lc[-1][:,0]) > 20:\n",
    "                    t, y, yerr = row_lc[-1][:,0], row_lc[-1][:,1], row_lc[-1][:,2]\n",
    "                    t, y, yerr, _, _, _, _, _ = clip_outliers(t, y, yerr, \n",
    "                                                              measurements_in_flux_units = self.lc_type == \"flux\", \n",
    "                                                              initial_clip=self.initial_clean_clip, clean_only=True)\n",
    "                    row_lc[-1] = np.vstack((t, y, yerr)).T\n",
    "                                  \n",
    "            light_curves.append(row_lc)\n",
    "        return light_curves\n",
    "\n",
    "    def get_spectra(self, rows):\n",
    "        \"\"\" Given df row(s), return the spectra as numpy arrays \"\"\"\n",
    "        spectra = []\n",
    "        for ind, row in rows.iterrows():\n",
    "            row_spectra = []\n",
    "            rowid = row[merge_key[self.use_bands[0]]]\n",
    "            rez = a.spec_df.query(f\"edr3_source_id == '{rowid}'\")\n",
    "            if len(rez) == 0:\n",
    "                spectra.append([])\n",
    "                continue\n",
    "            for si, spect in rez.iterrows():\n",
    "                filename = self.data_root /  self.lamost_spec_dir / spect[\"spec_filename\"]\n",
    "                if os.path.exists(filename):\n",
    "                    row_spectra.append(self._readLRSFits(filename))\n",
    "            spectra.append(row_spectra)\n",
    "        return spectra\n",
    "    \n",
    "    def _merge_bands(self):\n",
    "        if len(self.use_bands) == 1:\n",
    "            self.df = self.dfs[self.use_bands[0]]\n",
    "        elif len(self.use_bands) == 2 and \"v\" in self.use_bands and \"g\" in self.use_bands:\n",
    "            if self.verbose:\n",
    "                print(\"Merging bands...\", flush=True,end=\"\")\n",
    "            self.df = self.dfs[\"v\"].merge(self.dfs[\"g\"], how=self.merge_type, \n",
    "                                          left_on=merge_key[\"v\"], right_on=merge_key[\"g\"], suffixes=('_vband', '_gband'))\n",
    "            if self.verbose:\n",
    "                print(\"done.\", flush=True)\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Dont know how to merge these bands\")\n",
    "\n",
    "        if self.only_periodic:\n",
    "            try:\n",
    "                self.df = self.df[self.df[\"periodic\"]]\n",
    "            except:\n",
    "                print(\"No `periodic` column in the dataframe. Proceeding.\")\n",
    "\n",
    "        target_cols\n",
    "        # make a list of columns to save for bookkeeping\n",
    "        df_cols = self.df.columns\n",
    "        self.bookkeeping_all = []\n",
    "        for band in self.use_bands:\n",
    "            for col in bookkeeping_cols[band]:\n",
    "                if col in df_cols:\n",
    "                    self.bookkeeping_all.append(col)\n",
    "                elif f\"{col}_{band}band\" in df_cols:\n",
    "                    self.bookkeeping_all.append(f\"{col}_{band}band\")\n",
    "\n",
    "        self.metadata_all = []\n",
    "        for band in self.use_bands:\n",
    "            for col in metadata_cols[band]:\n",
    "                if col in df_cols:\n",
    "                    self.metadata_all.append(col)\n",
    "                elif f\"{col}_{band}band\" in df_cols:\n",
    "                    self.metadata_all.append(f\"{col}_{band}band\")\n",
    "\n",
    "        self.target_all = []\n",
    "        for band in self.use_bands:\n",
    "            self.target_all += target_cols[band]\n",
    "\n",
    "        self.target_lookup = {i: x for i,x in enumerate(np.unique(a.df[[target_cols[band][0] for band in a.use_bands]].values.ravel()))}\n",
    "        \n",
    "                    \n",
    "    def _readLRSFits(self, filename, z_corr=True):\n",
    "        # from https://github.com/fandongwei/pylamost\n",
    "        \n",
    "        hdulist = fits.open(filename)\n",
    "        len_list=len(hdulist)\n",
    "        if len_list == 1:\n",
    "            head = hdulist[0].header\n",
    "            scidata = hdulist[0].data\n",
    "            coeff0 = head['COEFF0']\n",
    "            coeff1 = head['COEFF1']\n",
    "            pixel_num = head['NAXIS1'] \n",
    "            specflux = scidata[0,]\n",
    "            ivar = scidata[1,]\n",
    "            spec_noconti = scidata[2,]\n",
    "            wavelength=np.linspace(0,pixel_num-1,pixel_num)\n",
    "            wavelength=np.power(10,(coeff0+wavelength*coeff1))\n",
    "            hdulist.close()\n",
    "        elif len_list == 2:\n",
    "            head = hdulist[0].header\n",
    "            scidata = hdulist[1].data\n",
    "            wavelength = scidata[0][2]\n",
    "            ivar = scidata[0][1]\n",
    "            specflux = scidata[0][0]\n",
    "\n",
    "        if z_corr:\n",
    "            try:\n",
    "                # correct for radial velocity of star\n",
    "                redshift = head['Z']\n",
    "            except:\n",
    "                redshift = 0.0\n",
    "            wavelength = wavelength - redshift * wavelength\n",
    "            \n",
    "        return np.vstack((wavelength, specflux, ivar)).T\n",
    "    \n",
    "    def _check_and_open_data_files(self):\n",
    "        \n",
    "        if len(self.use_bands) == 0:\n",
    "            raise Exception(\"Need a least one bandpass to use\")\n",
    "        if not os.path.isdir(self.data_root):\n",
    "            raise Exception(f\"{self.data_root} is not a valid data directory.\")\n",
    "        \n",
    "        dfs = {}\n",
    "        lcs = {}\n",
    "        for band in self.use_bands:\n",
    "            if not os.path.exists(self.data_root / raw_data_files[band][\"tab\"]):\n",
    "                raise Exception(f\"Missing tabular data for {band}.\")\n",
    "            if not os.path.exists(self.data_root / raw_data_files[band][\"lcs\"]):\n",
    "                raise Exception(f\"Missing light curve data for {band}.\")\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Opening {band} data files...\", flush=True, end=\"\")\n",
    "\n",
    "            dfs[band] = pd.read_csv(self.data_root / raw_data_files[band][\"tab\"])\n",
    "            if self.verbose:\n",
    "                print(f\" Found {len(dfs[band])} sources. \", end=\"\")\n",
    "                \n",
    "            lcs_file_type = \"\".join((self.data_root / raw_data_files[band][\"lcs\"]).suffixes)\n",
    "            \n",
    "            if lcs_file_type == \".zip\":\n",
    "                lcs[band] = (zipfile.ZipFile(self.data_root / raw_data_files[band][\"lcs\"]), \"zip\")\n",
    "            elif lcs_file_type in [\".tar.gz\", \".tgz\", \".tar\"]:\n",
    "                lcs[band] = (tarfile.open(self.data_root / raw_data_files[band][\"lcs\"], \"r\"), \"tar\")\n",
    "            else:\n",
    "                raise Exception(f\"Dont know how to open {self.data_root / raw_data_files[band]['lcs']}\")\n",
    "\n",
    "            if self.verbose:\n",
    "                print(\"done.\", flush=True)\n",
    "\n",
    "            self.lcs = lcs\n",
    "            self.dfs = dfs\n",
    "\n",
    "        if self.lamost_spec_file is not None and (self.data_root / self.lamost_spec_file).exists():\n",
    "            if self.verbose:\n",
    "                print(\"Opening spectra csv...\", flush=True, end=\"\")\n",
    "            self.spec_df = pd.read_csv(self.data_root / self.lamost_spec_file)\n",
    "            if self.verbose:\n",
    "                print(\"done.\", flush=True)\n",
    "            if self.only_sources_with_spectra:\n",
    "                sources_with_spectra = pd.unique(self.spec_df[\"edr3_source_id\"])\n",
    "                for band in self.use_bands:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Keeping only {band} band sources with spectra...\", flush=True, end=\"\")\n",
    "                    dfs[band] = dfs[band][dfs[band][merge_key[band]].isin(sources_with_spectra)]\n",
    "                    if self.verbose:\n",
    "                        print(f\" Left with {len(dfs[band])} sources. \", end=\"\")\n",
    "                        print(f\"done.\", flush=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "93620a2b-f87c-410d-8693-fcdc8fbe4996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening v data files... Found 687695 sources. done.\n",
      "Opening g data files... Found 378861 sources. done.\n",
      "Opening spectra csv...done.\n",
      "Keeping only v band sources with spectra... Left with 26412 sources. done.\n",
      "Keeping only g band sources with spectra... Left with 25965 sources. done.\n",
      "Merging bands...done.\n",
      "Priming tarballs by doing initial scan...done.\n"
     ]
    }
   ],
   "source": [
    "a=ASASSNVarStarDataset(Path(\"data/asaasn\"),10,verbose=True,only_periodic=True,\n",
    "                       recalc_period=False,prime=True,use_bands=[\"v\",\"g\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8d6e9522-683e-4b0f-94ab-c192fd23136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bookkeeping_data [27]\n",
      "classes (2,)\n",
      "lcs (167, 3), (209, 3)\n",
      "spectra (3879, 3), (3909, 3)\n",
      "phased (167, 3), (209, 3)\n",
      "metadata (89,)\n"
     ]
    }
   ],
   "source": [
    "a.return_items_as_list = False\n",
    "## what's the structure of what we just made?\n",
    "for k, v in a[0].items():\n",
    "    s = v[0]\n",
    "    if isinstance(s, (np.int64, int, float)):\n",
    "        rez = (1,)\n",
    "    elif isinstance(s, np.ndarray):\n",
    "        rez = s.shape\n",
    "    elif isinstance(s, list):\n",
    "        if len(s) == 0:\n",
    "            rez = \"None\"\n",
    "        else:\n",
    "            if isinstance(s[0], (tuple)):\n",
    "                rez = \", \".join(str(x.shape) for x in s[0]) \n",
    "            elif isinstance(s[0], (str, float, int)):\n",
    "                rez = f\"[{len(s)}]\"\n",
    "            else:\n",
    "                rez = \", \".join(str(x.shape) for x in s)\n",
    "                    \n",
    "    else:\n",
    "        rez = \"?\"\n",
    "    print(k, rez)\n",
    "a.return_items_as_list = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "20c2ef77-7e37-4e14-a751-62800675e6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rez = a[[0,1,3,4]]\n",
    "len(rez[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "00f84de9-0e1b-4fbf-9046-5184c939b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcs, metadata, spectra, targets, bookkeeping = a[[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d821193c-5d82-45cc-b59b-b4ac45f2c9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "len(batch[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ac1dd0b9-7a47-4f67-a0c9-14babbaaeab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7860f4b4-150b-4971-bda6-1bc2a6753625",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(a, batch_size=7, shuffle=True,collate_fn=collate_fn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "dc2cbb38-d011-409c-aac2-07cc2d872175",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, batch in enumerate(train_dataloader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "10365ed6-6732-4da5-8804-a844cb194048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "97a795a9-416a-4409-83a4-a9211534d14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[3][0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c595f62-0ed6-4924-bf8c-cd09c81f5313",
   "metadata": {},
   "source": [
    "## Spectra merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "1ac3f297-6d6e-498b-981a-002eac2a9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_id_df = pd.read_csv(\"merge_id.csv\")\n",
    "merge_id_df[\"index\"] = merge_id_df.index\n",
    "lamost_spec_df = pd.read_csv(\"data/asaasn/Spectra/27976.csv\", sep=\"|\")\n",
    "lamost_spec_df[\"inputobjs_input_id\"] = lamost_spec_df[\"inputobjs_input_id\"] - 1 # this is the iloc for the merge_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a581511d-77e0-40fc-840f-1e3f6ed0e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamost_spec_df = lamost_spec_df.merge(merge_id_df,left_on=\"inputobjs_input_id\",right_on=\"index\",how=\"left\")\n",
    "\n",
    "def make_spec_name(r):\n",
    "    return f'spec-{r[\"combined_lmjd\"]}-{r[\"combined_planid\"]}_sp{r[\"combined_spid\"]:02}-{r[\"combined_fiberid\"]:03}.fits.gz'\n",
    "\n",
    "lamost_spec_df['spec_filename'] = lamost_spec_df.apply(make_spec_name, axis=1)\n",
    "lamost_spec_df.to_csv(\"data/asaasn/Spectra/lamost_spec.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "cc549027-1d3a-4800-aed5-7e8d73ce48fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",inputobjs_input_id,inputobjs_input_ra,inputobjs_input_dec,inputobjs_dist,combined_obsid,combined_obsdate,combined_lmjd,combined_mjd,combined_planid,combined_spid,combined_fiberid,combined_class,combined_subclass,combined_z,combined_ra,combined_dec,combined_feh,combined_logg,combined_rv,combined_teff,raj2000,dej2000,edr3_source_id,index,spec_filename\n",
      "0,8,280.33355,44.91092,1.6667680014096964,457215016,2016-05-07,57516,57515,HD184435N434959B01,15,16,STAR,F6,1.44433e-05,280.3341642,44.9110786,-0.221,4.223,4.33,6113.43,280.33355,44.91092,EDR3 2117581746386552576,8,spec-57516-HD184435N434959B01_sp15-016.fits.gz\n",
      "1,25,352.58827,21.71119,0.2740173706497962,260211063,2014-11-05,56967,56966,EG232823N195308V01,11,63,STAR,G3,1.93467e-05,352.588347,21.711164,0.132,4.169,5.8,5849.0,352.58827,21.71119,EDR3 2827427930745553152,25,spec-56967-EG232823N195308V01_sp11-063.fits.gz\n",
      "2,43,119.68567,15.35703,1.9002163538280932,864113050,2020-12-08,59192,59191,HD074946N150623V01,13,50,STAR,A6,0.0001126112,119.686015,15.3574398,-0.368,4.081,33.76,7810.8,119.68567,15.35703,EDR3 654686331696211456,43,spec-59192-HD074946N150623V01_sp13-050.fits.gz\n",
      "3,43,119.68567,15.35703,1.5274012990204695,121303222,2013-02-15,56339,56338,GAC120N14V1,3,222,STAR,A5,9.76009e-05,119.68594,15.357365,-0.318,4.048,29.26,7764.94,119.68567,15.35703,EDR3 654686331696211456,43,spec-56339-GAC120N14V1_sp03-222.fits.gz\n",
      "4,43,119.68567,15.35703,1.9002163538280932,546002030,2017-02-28,57813,57812,HD080327N165051V01,2,30,STAR,A6,0.0001671156,119.686015,15.3574398,-0.29,4.146,50.1,7781.77,119.68567,15.35703,EDR3 654686331696211456,43,spec-57813-HD080327N165051V01_sp02-030.fits.gz\n",
      "5,59,116.86418,43.66558,0.4751139616076973,410502117,2016-01-22,57410,57409,HD075322N454428V02,2,117,STAR,F5,0.0001290559,116.8643617,43.6655919,0.019,4.25,38.69,6400.03,116.86418,43.66558,EDR3 925880840326498432,59,spec-57410-HD075322N454428V02_sp02-117.fits.gz\n",
      "6,62,55.58999,24.66444,1.119857341385297,100903128,2013-01-02,56295,56294,VB056N24V1,3,128,STAR,dM0,3.491e-05,55.589748,24.66466,,5.215,,3922.61,55.58999,24.66444,EDR3 68363411064150272,62,spec-56295-VB056N24V1_sp03-128.fits.gz\n",
      "7,62,55.58999,24.66444,1.0765385096265494,837014217,2020-09-18,59111,59110,KII035052N235741B02,14,217,STAR,dM0,-1.528e-05,55.589772,24.664664,,4.833,,3733.34,55.58999,24.66444,EDR3 68363411064150272,62,spec-59111-KII035052N235741B02_sp14-217.fits.gz\n",
      "8,62,55.58999,24.66444,1.135003788717179,841714217,2020-10-16,59139,59138,TD035052N235741K01,14,217,STAR,dM0,-3.371e-05,55.5897552,24.6646721,,5.04,,3879.46,55.58999,24.66444,EDR3 68363411064150272,62,spec-59139-TD035052N235741K01_sp14-217.fits.gz\n"
     ]
    }
   ],
   "source": [
    "!head data/asaasn/Spectra/lamost_spec.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "7fc04391-f59a-4993-9a0a-cc667dc822c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, chunk in enumerate(np.array_split(a.df[[\"raj2000\",\"dej2000\",\"edr3_source_id\"]], 5)):\n",
    "    chunk.to_csv(f'merge_a.{idx}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "44631370-699a-4115-989e-a621d3c0b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = a.df[[\"raj2000\",\"dej2000\",\"edr3_source_id\"]]\n",
    "# tmp[\"sep\"] = 2.0\n",
    "tmp.to_csv(f'merge_id.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0e05f56e-5e14-4ab8-beaa-44b86120abb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raj2000,dej2000,edr3_source_id\n",
      "258.64478,30.29155,EDR3 1333085560085564544\n",
      "293.81352,23.63857,EDR3 2021010643745700608\n",
      "310.79987,67.24633,EDR3 2246124001521354368\n",
      "311.80428,19.52859,EDR3 1814028193934473728\n",
      "278.76235,-36.44085,EDR3 6733359322482079744\n",
      "145.10705,-41.66644,EDR3 5425390701060256640\n",
      "328.50332,45.58807,EDR3 1973594032944176640\n",
      "128.78229,-46.19009,EDR3 5521855975772074240\n",
      "280.33355,44.91092,EDR3 2117581746386552576\n"
     ]
    }
   ],
   "source": [
    "!head merge_id.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi_modal",
   "language": "python",
   "name": "multi_modal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
