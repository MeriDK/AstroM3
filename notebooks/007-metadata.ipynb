{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aaaeb1c-ae3f-4805-bcda-fe50882b0d96",
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c5cce77-627a-48ff-a7f3-1fb6d936a691",
   "metadata": {},
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af3fab9-27ea-4b8d-beb9-8c7393d2f0d9",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "We used 16 features from Gaia EDR3, 2MASS and AllWISE. These\n",
    "include the EDR3 G, BP, RP magnitudes and the associated uncertainties, the 𝐵𝑃−𝑅𝑃 color, the 𝐵𝑃−𝑅𝑃 excess factor, signal-to-noise\n",
    "ratios in G and BP, the renormalized unit weight error (RUWE), the\n",
    "𝐽 − 𝐾𝑠 color, the absolute 𝑊𝑅𝑃 magnitude and the absolute 𝑊𝐽 𝐾\n",
    "magnitude. The EDR3 signal-to-noise ratios are essentially the ratio of the observed flux divided by the error in the flux. As noted\n",
    "earlier, the EDR3 photometric uncertainties and flux errors encode\n",
    "information about the photometric variability of stars. We also used\n",
    "the absolute, “reddening-free” Wesenheit magnitudes (Madore 1982;\n",
    "Lebzelter et al. 2018)\n",
    "𝑊𝑅𝑃 = 𝑀RP − 1.3(𝐵𝑃 − 𝑅𝑃) , (1)\n",
    "and\n",
    "𝑊𝐽 𝐾 = 𝑀Ks − 0.686(𝐽 − 𝐾𝑠) (2)\n",
    "and the probabilistic EDR3 distances from Bailer-Jones et al. (2021)\n",
    "\"\"\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ac94e67e-f150-4d23-9dac-fef5b5d58e08",
   "metadata": {},
   "source": [
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = []\n",
    "    total_correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for X, y in tqdm(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "        _, predicted_labels = torch.max(probabilities, dim=1)\n",
    "        correct_predictions = (predicted_labels == y).sum().item()\n",
    "\n",
    "        total_correct_predictions += correct_predictions\n",
    "        total_predictions += y.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return sum(total_loss) / len(total_loss), total_correct_predictions / total_predictions\n",
    "    \n",
    "def val_epoch():\n",
    "    model.eval()\n",
    "    total_loss = []\n",
    "    total_correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(val_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y)\n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "            _, predicted_labels = torch.max(probabilities, dim=1)\n",
    "            correct_predictions = (predicted_labels == y).sum().item()\n",
    "    \n",
    "            total_correct_predictions += correct_predictions\n",
    "            total_predictions += y.size(0)\n",
    "\n",
    "    return sum(total_loss) / len(total_loss), total_correct_predictions / total_predictions"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "327d590d-5215-486f-852a-5787ab6d5e11",
   "metadata": {},
   "source": [
    "METADATA_COLS = [\n",
    "    'mean_vmag', 'amplitude', 'period', 'phot_g_mean_mag', 'e_phot_g_mean_mag', 'lksl_statistic',\n",
    "    'rfr_score', 'phot_bp_mean_mag', 'e_phot_bp_mean_mag', 'phot_rp_mean_mag', 'e_phot_rp_mean_mag',\n",
    "    'bp_rp', 'parallax', 'parallax_error', 'parallax_over_error', 'pmra', 'pmra_error', 'pmdec',\n",
    "    'pmdec_error', 'j_mag', 'e_j_mag', 'h_mag', 'e_h_mag', 'k_mag', 'e_k_mag', 'w1_mag', 'e_w1_mag',\n",
    "    'w2_mag', 'e_w2_mag', 'w3_mag', 'w4_mag', 'j_k', 'w1_w2', 'w3_w4', 'pm', 'ruwe'\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "59951ca8-e978-4b07-8212-2431dd2365c4",
   "metadata": {},
   "source": [
    "class MetaVDataset(Dataset):\n",
    "    def __init__(self, file, split='train', classes=None, min_samples=None, max_samples=None,\n",
    "                 random_seed=42, verbose=True):\n",
    "        self.df = pd.read_csv(file)\n",
    "        self.metadata_cols = METADATA_COLS\n",
    "        self.df = self.df[self.metadata_cols + ['edr3_source_id', 'variable_type']]\n",
    "\n",
    "        self.split = split\n",
    "        self.verbose = verbose\n",
    "        self.classes = classes\n",
    "        self.min_samples = min_samples\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        self._drop_nan()\n",
    "        self._drop_duplicates()\n",
    "        self._filter_classes()\n",
    "        self._limit_samples()\n",
    "        self._split()\n",
    "        self._normalize()\n",
    "\n",
    "        self.id2target = {i: x for i, x in enumerate(sorted(self.df['variable_type'].unique()))}\n",
    "        self.target2id = {v: k for k, v in self.id2target.items()}\n",
    "        self.num_classes = len(self.id2target)\n",
    "\n",
    "    def _drop_nan(self):\n",
    "        if self.verbose:\n",
    "            print('Dropping nan values...', end=' ')\n",
    "\n",
    "        self.df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'Done. Left with {len(self.df)} rows.')\n",
    "\n",
    "    def _drop_duplicates(self):\n",
    "        if self.verbose:\n",
    "            print('Dropping duplicated values...', end=' ')\n",
    "\n",
    "        self.df.drop_duplicates(subset=['edr3_source_id'], keep='last', inplace=True)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'Done. Left with {len(self.df)} rows.')\n",
    "\n",
    "    def _filter_classes(self):\n",
    "        if self.classes:\n",
    "            if self.verbose:\n",
    "                print(f'Leaving only classes: {self.classes}... ', end='')\n",
    "\n",
    "            self.df = self.df[self.df['variable_type'].isin(self.classes)]\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'{len(self.df)} objects left.')\n",
    "\n",
    "    def _limit_samples(self):\n",
    "        if self.max_samples or self.min_samples:\n",
    "            if self.verbose:\n",
    "                print(f'Removing objects that have more than {self.max_samples} or less than {self.min_samples} '\n",
    "                      f'samples... ', end='')\n",
    "\n",
    "            value_counts = self.df['variable_type'].value_counts()\n",
    "\n",
    "            if self.min_samples:\n",
    "                classes_to_remove = value_counts[value_counts < self.min_samples].index\n",
    "                self.df = self.df[~self.df['variable_type'].isin(classes_to_remove)]\n",
    "\n",
    "            if self.max_samples:\n",
    "                classes_to_limit = value_counts[value_counts > self.max_samples].index\n",
    "                for class_type in classes_to_limit:\n",
    "                    class_indices = self.df[self.df['variable_type'] == class_type].index\n",
    "                    indices_to_keep = np.random.choice(class_indices, size=self.max_samples, replace=False)\n",
    "                    self.df = self.df.drop(index=set(class_indices) - set(indices_to_keep))\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'{len(self.df)} objects left.')\n",
    "\n",
    "    def _split(self):\n",
    "        unique_ids = self.df['edr3_source_id'].unique()\n",
    "        train_ids, temp_ids = train_test_split(unique_ids, test_size=0.2, random_state=self.random_seed)\n",
    "        val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=self.random_seed)\n",
    "\n",
    "        if self.split == 'train':\n",
    "            self.df = self.df[self.df['edr3_source_id'].isin(train_ids)]\n",
    "        elif self.split == 'val':\n",
    "            self.df = self.df[self.df['edr3_source_id'].isin(val_ids)]\n",
    "        elif self.split == 'test':\n",
    "            self.df = self.df[self.df['edr3_source_id'].isin(test_ids)]\n",
    "        else:\n",
    "            print('Split is not train, val, or test. Keeping the whole dataset')\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'{self.split} split is selected: {len(self.df)} objects left.')\n",
    "\n",
    "    def _normalize(self):\n",
    "        if self.split == 'train':\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(self.df[self.metadata_cols])\n",
    "            joblib.dump(self.scaler, 'scaler.pkl')\n",
    "        else:\n",
    "            self.scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "        self.df[self.metadata_cols] = self.scaler.transform(self.df[self.metadata_cols])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        el = self.df.iloc[idx]\n",
    "        X = el[self.metadata_cols].values.astype(np.float32)\n",
    "        y = self.target2id[el['variable_type']]\n",
    "\n",
    "        return X, y"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ecd4b210-6235-4ec8-b69f-0dc3b39beda4",
   "metadata": {},
   "source": [
    "class MetaClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=36, hidden_dim=128, num_classes=15):\n",
    "        super(MetaClassifier, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e6dac67d-f727-4527-b857-b3adc86a1f55",
   "metadata": {},
   "source": [
    "file = '/home/mariia/AstroML/data/asassn/asassn_catalog_full.csv'\n",
    "train_dataset = MetaVDataset(file, split='train', classes=None, min_samples=5000, max_samples=20000, random_seed=42, verbose=True)\n",
    "val_dataset = MetaVDataset(file, split='val', classes=None, min_samples=5000, max_samples=20000, random_seed=42, verbose=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7d3781b7-7c2a-4707-a81f-81fa80198dc9",
   "metadata": {},
   "source": [
    "train_dataset[4]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9854ac52-ebcd-4b04-b711-98c6e5bbd895",
   "metadata": {},
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5219b9e2-de02-41ef-9635-b83f000a73c9",
   "metadata": {},
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using', device)\n",
    "\n",
    "model = MetaClassifier(num_classes=train_dataset.num_classes)\n",
    "model = model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [], []"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "62377af0-fdba-424e-957b-be084609c032",
   "metadata": {},
   "source": [
    "for i in range(20):\n",
    "    print(f'Epoch {i}')\n",
    "    \n",
    "    train_loss, train_acc = train_epoch()\n",
    "    print(f'Train Loss: {round(train_loss, 3)} Acc: {round(train_acc, 2)}')\n",
    "    \n",
    "    val_loss, val_acc = val_epoch()\n",
    "    print(f'Val Loss: {round(val_loss, 3)} Acc: {round(val_acc, 2)}')\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "06bf5c1f-92ef-4175-b399-2789db5a05fa",
   "metadata": {},
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot training and validation losses on the left side\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(val_losses, label='Validation Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot training and validation accuracies on the right side\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(val_accs, label='Validation Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "56356cb9-0d23-42eb-a015-7efd70c633d0",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "\n",
    "all_true_labels = []\n",
    "all_predicted_labels = []\n",
    "\n",
    "for X, y in tqdm(val_dataloader):\n",
    "    with torch.no_grad():\n",
    "        X = X.to(device)\n",
    "\n",
    "        logits = model(X)\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "        _, predicted_labels = torch.max(probabilities, dim=1)\n",
    "\n",
    "        all_true_labels.extend(y.numpy())\n",
    "        all_predicted_labels.extend(predicted_labels.cpu().numpy())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "80d9888b-f421-4fab-b24d-09750d7bb715",
   "metadata": {},
   "source": [
    "sum([all_true_labels[i] == all_predicted_labels[i] for i in range(len(all_predicted_labels))])/len(all_predicted_labels)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a5b58e97-c2d0-4a2e-abdb-6cf137ea6079",
   "metadata": {},
   "source": [
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "\n",
    "# Calculate percentage values for confusion matrix\n",
    "conf_matrix_percent = 100 * conf_matrix / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Get the labels from the id2target mapping\n",
    "labels = [val_dataset.id2target[i] for i in range(len(conf_matrix))]\n",
    "\n",
    "# Plot both confusion matrices side by side\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 7))\n",
    "\n",
    "# Plot absolute values confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "axes[0].set_title('Confusion Matrix - Absolute Values')\n",
    "\n",
    "# Plot percentage values confusion matrix\n",
    "sns.heatmap(conf_matrix_percent, annot=True, fmt='.0f', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "axes[1].set_title('Confusion Matrix - Percentages')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b56a2-8645-4967-8ac4-4c39d3115586",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e91e36-8a18-4ec7-9841-f3df7aafab55",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21446de-688e-4dc4-91f9-1ba33ffa888a",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719207da-9398-4987-892b-6c9f01857b08",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f3e5c-d75c-4a98-9f41-98b55c80b8e5",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7caf8e-9a83-4783-9caa-d00d834e6062",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0204ec6-73b5-42de-b084-a0e63bb19202",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed5650-d97f-4747-ab4b-bb3d09c5f4fa",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da6c31-3376-4844-a1da-d56d5ade05ec",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad54ac9c-78ad-46b5-b1ea-526ba044e139",
   "metadata": {},
   "source": [
    "METADATA_COLS = [\n",
    "    'mean_vmag', 'amplitude', 'period', 'phot_g_mean_mag', 'e_phot_g_mean_mag', 'lksl_statistic',\n",
    "    'rfr_score', 'phot_bp_mean_mag', 'e_phot_bp_mean_mag', 'phot_rp_mean_mag', 'e_phot_rp_mean_mag',\n",
    "    'bp_rp', 'parallax', 'parallax_error', 'parallax_over_error', 'pmra', 'pmra_error', 'pmdec',\n",
    "    'pmdec_error', 'j_mag', 'e_j_mag', 'h_mag', 'e_h_mag', 'k_mag', 'e_k_mag', 'w1_mag', 'e_w1_mag',\n",
    "    'w2_mag', 'e_w2_mag', 'w3_mag', 'w4_mag', 'j_k', 'w1_w2', 'w3_w4', 'pm', 'ruwe'\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "715745a8-06b3-47c7-ac1d-77807616bbc9",
   "metadata": {},
   "source": [
    "df = pd.read_csv('/home/mariia/AstroML/data/asassn/asassn_catalog_full.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e048b061-91b0-491b-880d-e793bfc1822b",
   "metadata": {},
   "source": [
    "metadata_cols = ['mean_vmag', 'amplitude', 'period', 'phot_g_mean_mag', 'e_phot_g_mean_mag', 'lksl_statistic', 'rfr_score', 'phot_bp_mean_mag', 'e_phot_bp_mean_mag', \n",
    "                 'phot_rp_mean_mag', 'e_phot_rp_mean_mag', 'bp_rp', 'parallax', 'parallax_error', 'parallax_over_error', 'pmra', 'pmra_error', 'pmdec', 'pmdec_error', \n",
    "                 'j_mag', 'e_j_mag', 'h_mag', 'e_h_mag', 'k_mag', 'e_k_mag', 'w1_mag', 'e_w1_mag', 'w2_mag', 'e_w2_mag', 'w3_mag', 'e_w3_mag', 'w4_mag', 'e_w4_mag', \n",
    "                 'j_k', 'w1_w2', 'w3_w4', 'apass_vmag', 'e_apass_vmag', 'apass_bmag', 'e_apass_bmag', 'apass_gpmag', 'e_apass_gpmag', 'apass_rpmag', 'e_apass_rpmag', \n",
    "                 'apass_ipmag', 'e_apass_ipmag', 'FUVmag', 'e_FUVmag', 'NUVmag', 'e_NUVmag', 'pm', 'ruwe']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dde7d8d-2140-4985-ace3-9278d3b1ba58",
   "metadata": {},
   "source": [
    "df = df[metadata_cols + ['edr3_source_id', 'variable_type']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "884a1e8d-4584-4997-b786-f60627598832",
   "metadata": {},
   "source": [
    "df['variable_type'].value_counts().head(30)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab1fd963-a3e7-4301-8633-4c2211de70d0",
   "metadata": {},
   "source": [
    "df = df[METADATA_COLS]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "663970fd-c60f-446b-9ee8-21746aca8b3b",
   "metadata": {},
   "source": [
    "df.isna().sum() / len(df) * 100"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbb8cb7a-21eb-44a9-b0be-ec861b10efc3",
   "metadata": {},
   "source": [
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "087801a7-33ff-4dce-a026-03add1e81e3b",
   "metadata": {},
   "source": [
    "df.dropna(axis=0, how='any', inplace=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b00829-dd88-4abe-a9c3-a183e74dd237",
   "metadata": {},
   "source": [
    "class MetaVDataset(Dataset):\n",
    "    def __init__(self, file, split='train', scales=None, classes=None, min_samples=None, max_samples=None, random_seed=42, verbose=True):\n",
    "        self.df = pd.read_csv(file)\n",
    "        self.split = split\n",
    "        self.random_sample = random_sample\n",
    "        self.verbose = verbose\n",
    "        self.scales = scales\n",
    "        self.classes = classes\n",
    "        self.min_samples = min_samples\n",
    "        self.max_samples = max_samples\n",
    "\n",
    "        self._filter_classes()\n",
    "        self._limit_samples()\n",
    "        self._split()\n",
    "        self._normalize()\n",
    "\n",
    "        self.id2target = {i: x for i, x in enumerate(sorted(self.df['variable_type'].unique()))}\n",
    "        self.target2id = {v: k for k, v in self.id2target.items()}\n",
    "        self.num_classes = len(self.id2target)\n",
    "        \n",
    "    def _filter_classes(self):\n",
    "        if self.classes:\n",
    "            if self.verbose:\n",
    "                print(f'Leaving only classes: {self.classes}... ', end='')\n",
    "\n",
    "            self.df = self.df[self.df['variable_type'].isin(self.classes)]\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'{len(self.df)} objects left.')\n",
    "\n",
    "    def _limit_samples(self):\n",
    "        if self.max_samples or self.min_samples:\n",
    "            if self.verbose:\n",
    "                print(f'Removing objects that have more than {self.max_samples} or less than {self.min_samples} samples... ', end='')\n",
    "\n",
    "            value_counts = self.df['variable_type'].value_counts()\n",
    "\n",
    "            if self.min_samples:\n",
    "                classes_to_remove = value_counts[value_counts < self.min_samples].index\n",
    "                self.df = self.df[~self.df['variable_type'].isin(classes_to_remove)]\n",
    "\n",
    "            if self.max_samples:\n",
    "                classes_to_limit = value_counts[value_counts > self.max_samples].index\n",
    "                for class_type in classes_to_limit:\n",
    "                    class_indices = self.df[self.df['variable_type'] == class_type].index\n",
    "                    indices_to_keep = np.random.choice(class_indices, size=self.max_samples, replace=False)\n",
    "                    self.df = self.df.drop(index=set(class_indices) - set(indices_to_keep))\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'{len(self.df)} objects left.')\n",
    "\n",
    "    def _split(self):\n",
    "        unique_ids = self.df['edr3_source_id'].unique()\n",
    "        train_ids, temp_ids = train_test_split(unique_ids, test_size=0.2, random_state=self.random_seed)\n",
    "        val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=self.random_seed)\n",
    "\n",
    "        if self.split == 'train':\n",
    "            self.df = self.df[self.df['edr3_source_id'].isin(train_ids)]\n",
    "        elif self.split == 'val':\n",
    "            self.df = self.df[self.df['edr3_source_id'].isin(val_ids)]\n",
    "        elif self.split == 'test':\n",
    "            self.df = self.df[self.df['edr3_source_id'].isin(test_ids)]\n",
    "        else:\n",
    "            print('Split is not train, val, or test. Keeping the whole dataset')\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'{self.split} split is selected: {len(self.df)} objects left.')\n",
    "\n",
    "    def _normalize(self):\n",
    "        if self.split in ('val', 'test') and self.scales is None:\n",
    "            raise Error('Scales must be provided for val/test splits')\n",
    "\n",
    "        pass\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        el = self.df.iloc[idx]\n",
    "\n",
    "        X = self.get_vlc(el['name']) if el['band'] == 'v' else self.get_glc(el['name'])\n",
    "        X, mask = self.preprocess(X, el['period'], el['band'])\n",
    "        y = self.target2id[el['target']]\n",
    "\n",
    "        return X, mask, y"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14800fcd-4233-457c-9fb0-b45269fece3c",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a6769-0c3e-4fb9-98cf-a091e68b1d57",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50acfc-bf08-40d8-ba87-c7a7da90753d",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07839deb-2bd7-47c7-8d5f-89b262871dfb",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb00c2-f122-4445-be21-d1de1c848bb8",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8513ec-0d0b-479c-bf99-9c2bc078cbe0",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853cd7d0-fa01-4141-8f65-e9dec7713645",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b064b4f-0660-4a97-b5d3-b55e877c16d6",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00492ab8-1c71-4eae-919a-5f96b2407612",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53975c-ea09-490d-a533-4df9af7f0301",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d164354-71bd-4199-9e14-841241813fe7",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5f09f-fc56-4e08-b586-55fe06614bb8",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc02f433-7aab-4cb2-9753-cb64221d865e",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95a464-3209-4e8d-be45-c12e608516d4",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283e4f25-2ec7-46d5-88b6-7ed46c1261c1",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c296b1-2610-4ddb-83fc-b23e948b25fb",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4d5cb-09ff-4158-9f7f-076fafe824a5",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa4bc3c-cdbe-481a-85e3-9bf71f3ae75c",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
