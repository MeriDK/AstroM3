{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d968ecd-0bc1-4655-a05b-b2e6595fb823",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from astropy.io import fits\n",
    "import json\n",
    "import os\n",
    "from io import BytesIO\n",
    "from scipy import stats\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n",
    "from util.parallelzipfile import ParallelZipFile as ZipFile\n",
    "from util.preprocess_data import clip_outliers\n",
    "from models.Informer import DataEmbedding, EncoderLayer, AttentionLayer, ProbAttention, Encoder"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5cfbb13-6519-4881-9d96-d29cfe560ee1",
   "metadata": {},
   "source": [
    "METADATA_COLS = [\n",
    "    'mean_vmag', 'amplitude', 'period', 'phot_g_mean_mag', 'e_phot_g_mean_mag', 'lksl_statistic',\n",
    "    'rfr_score', 'phot_bp_mean_mag', 'e_phot_bp_mean_mag', 'phot_rp_mean_mag', 'e_phot_rp_mean_mag',\n",
    "    'bp_rp', 'parallax', 'parallax_error', 'parallax_over_error', 'pmra', 'pmra_error', 'pmdec',\n",
    "    'pmdec_error', 'j_mag', 'e_j_mag', 'h_mag', 'e_h_mag', 'k_mag', 'e_k_mag', 'w1_mag', 'e_w1_mag',\n",
    "    'w2_mag', 'e_w2_mag', 'w3_mag', 'w4_mag', 'j_k', 'w1_w2', 'w3_w4', 'pm', 'ruwe'\n",
    "]\n",
    "\n",
    "CLASSES = ['CWA', 'CWB', 'DCEP', 'DCEPS', 'DSCT', 'EA', 'EB', 'EW',\n",
    "           'HADS', 'M', 'ROT', 'RRAB', 'RRC', 'RRD', 'RVA', 'SR']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d79a8905-b0ca-4dce-9a5d-ee6bd3f357d3",
   "metadata": {},
   "source": [
    "class VPSMDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 # general\n",
    "                 data_root='/home/mariia/AstroML/data/asassn/', file='spectra_v_merged_fixed.csv', split='train',\n",
    "                 min_samples=None, max_samples=None, classes=None, random_seed=42, verbose=True,\n",
    "\n",
    "                 # photometry\n",
    "                 v_zip='asassnvarlc_vband_complete.zip', v_prefix='vardb_files', seq_len=200,\n",
    "                 phased=False, periodic=False, clip=False, aux=False,\n",
    "\n",
    "                 # spectra\n",
    "                 lamost_spec_dir='Spectra/v2', spectra_v_file='spectra_v_merged.csv', z_corr=False):\n",
    "\n",
    "        self.data_root = data_root\n",
    "        self.df = pd.read_csv(os.path.join(data_root, file))\n",
    "        self.metadata_cols = METADATA_COLS\n",
    "        self.all_cols = self.metadata_cols + ['edr3_source_id', 'variable_type', 'spec_filename', 'asassn_name']\n",
    "        self.df = self.df[self.all_cols]\n",
    "\n",
    "        self.reader_v = ZipFile(os.path.join(data_root, v_zip))\n",
    "        self.v_prefix = v_prefix\n",
    "\n",
    "        self.lamost_spec_dir = os.path.join(data_root, lamost_spec_dir)\n",
    "        self.spectra_v_file = os.path.join(data_root, spectra_v_file)\n",
    "        self.z_corr = z_corr\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.split = split\n",
    "        self.min_samples = min_samples\n",
    "        self.max_samples = max_samples\n",
    "        self.classes = classes\n",
    "        self.phased = phased\n",
    "        self.periodic = periodic\n",
    "        self.clip = clip\n",
    "        self.aux = aux\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        self._drop_nan()\n",
    "        self._filter_classes()\n",
    "        self._limit_samples()\n",
    "        self._split()\n",
    "        self._normalize_metadata()\n",
    "\n",
    "        self.id2target = {i: x for i, x in enumerate(sorted(self.df['variable_type'].unique()))}\n",
    "        self.target2id = {v: k for k, v in self.id2target.items()}\n",
    "        self.num_classes = len(self.id2target)\n",
    "\n",
    "    def _drop_nan(self):\n",
    "        if self.verbose:\n",
    "            print('Dropping nan values...', end=' ')\n",
    "\n",
    "        self.df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'Done. Left with {len(self.df)} rows.')\n",
    "\n",
    "    def _filter_classes(self):\n",
    "        if self.classes:\n",
    "            if self.verbose:\n",
    "                print(f'Leaving only classes: {self.classes}... ', end='')\n",
    "\n",
    "            self.df = self.df[self.df['variable_type'].isin(self.classes)]\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'{len(self.df)} objects left.')\n",
    "\n",
    "    def _limit_samples(self):\n",
    "        if self.max_samples or self.min_samples:\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f'Removing objects that have more than {self.max_samples} or less than {self.min_samples} samples... ',\n",
    "                    end='')\n",
    "\n",
    "            value_counts = self.df['variable_type'].value_counts()\n",
    "\n",
    "            if self.min_samples:\n",
    "                classes_to_remove = value_counts[value_counts < self.min_samples].index\n",
    "                self.df = self.df[~self.df['variable_type'].isin(classes_to_remove)]\n",
    "\n",
    "            if self.max_samples:\n",
    "                classes_to_limit = value_counts[value_counts > self.max_samples].index\n",
    "                for class_type in classes_to_limit:\n",
    "                    class_indices = self.df[self.df['variable_type'] == class_type].index\n",
    "                    indices_to_keep = np.random.choice(class_indices, size=self.max_samples, replace=False)\n",
    "                    self.df = self.df.drop(index=set(class_indices) - set(indices_to_keep))\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'{len(self.df)} objects left.')\n",
    "\n",
    "    def _split(self):\n",
    "        unique_ids = self.df['edr3_source_id'].unique()\n",
    "        train_ids, temp_ids = train_test_split(unique_ids, test_size=0.2, random_state=self.random_seed)\n",
    "        val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=self.random_seed)\n",
    "\n",
    "        if self.split == 'train':\n",
    "            self.df = self.df[self.df['edr3_source_id'].isin(train_ids)]\n",
    "        elif self.split == 'val':\n",
    "            self.df = self.df[self.df['edr3_source_id'].isin(val_ids)]\n",
    "        elif self.split == 'test':\n",
    "            self.df = self.df[self.df['edr3_source_id'].isin(test_ids)]\n",
    "        else:\n",
    "            print('Split is not train, val, or test. Keeping the whole dataset')\n",
    "\n",
    "        self.df = self.df.reset_index()\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'{self.split} split is selected: {len(self.df)} objects left.')\n",
    "\n",
    "    def _normalize_metadata(self):\n",
    "        if self.split == 'train':\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(self.df[self.metadata_cols])\n",
    "            joblib.dump(self.scaler, 'scaler.pkl')\n",
    "        else:\n",
    "            self.scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "        self.df[self.metadata_cols] = self.scaler.transform(self.df[self.metadata_cols])\n",
    "\n",
    "    def get_vlc(self, file_name):\n",
    "        csv = BytesIO()\n",
    "        file_name = file_name.replace(' ', '')\n",
    "        data_path = f'{self.v_prefix}/{file_name}.dat'\n",
    "\n",
    "        csv.write(self.reader_v.read(data_path))\n",
    "        csv.seek(0)\n",
    "\n",
    "        lc = pd.read_csv(csv, sep='\\s+', skiprows=2, names=['HJD', 'MAG', 'MAG_ERR', 'FLUX', 'FLUX_ERR'],\n",
    "                         dtype={'HJD': float, 'MAG': float, 'MAG_ERR': float, 'FLUX': float, 'FLUX_ERR': float})\n",
    "\n",
    "        return lc[['HJD', 'FLUX', 'FLUX_ERR']].values\n",
    "\n",
    "    def preprocess_lc(self, X, period, band):\n",
    "        # 2 sort based on HJD\n",
    "        sorted_indices = np.argsort(X[:, 0])\n",
    "        X = X[sorted_indices]\n",
    "\n",
    "        # 3 clip outliers\n",
    "        # TODO double check clip outliers function\n",
    "        if self.clip:\n",
    "            t, y, y_err = X[:, 0], X[:, 1], X[:, 2]\n",
    "            if len(t) > 20:\n",
    "                t, y, y_err, _, _, _, _, _ = clip_outliers(t, y, y_err, measurements_in_flux_units=True,\n",
    "                                                           initial_clip=(20, 5), clean_only=True)\n",
    "            X = np.vstack((t, y, y_err)).T\n",
    "\n",
    "        # Calculate min max before normalization\n",
    "        log_abs_min = 0 if min(X[:, 1]) == 0 else np.log(abs(min(X[:, 1])))\n",
    "        log_abs_max = np.log(abs(max(X[:, 1])))\n",
    "\n",
    "        # 4 normalize\n",
    "        mean = X[:, 1].mean()\n",
    "        std = stats.median_abs_deviation(X[:, 1])\n",
    "        X[:, 0] = (X[:, 0] - X[:, 0].min()) / (X[:, 0].max() - X[:, 0].min())\n",
    "        X[:, 1] = (X[:, 1] - mean) / std\n",
    "        X[:, 2] = X[:, 2] / std\n",
    "\n",
    "        # 5 trim if longer than seq_len\n",
    "        if X.shape[0] > self.seq_len:\n",
    "            start = np.random.randint(0, len(X) - self.seq_len)\n",
    "            X = X[start:start + self.seq_len, :]\n",
    "\n",
    "            # if self.split == 'train':\n",
    "            #     start = np.random.randint(0, len(X) - self.seq_len)\n",
    "            #     X = X[start:start + self.seq_len, :]\n",
    "            # else:\n",
    "            #     X = X[:self.seq_len, :]\n",
    "\n",
    "        # 1 phase\n",
    "        if self.phased:\n",
    "            X = np.vstack(((X[:, 0] % period) / period, X[:, 1], X[:, 2])).T\n",
    "\n",
    "        # pad if needed and create mask\n",
    "        mask = np.ones(self.seq_len)\n",
    "        if X.shape[0] < self.seq_len:\n",
    "            mask[X.shape[0]:] = 0\n",
    "            X = np.pad(X, ((0, self.seq_len - X.shape[0]), (0, 0)), 'constant', constant_values=(0,))\n",
    "\n",
    "        # add aux\n",
    "        if self.aux:\n",
    "            log_abs_mean = np.log(abs(mean))\n",
    "            log_std = np.log(std)\n",
    "            log_period = 0 if pd.isna(period) else np.log(period)\n",
    "\n",
    "            # aux = np.tile([log_abs_min, log_abs_max, log_abs_mean, log_std, log_period], (self.seq_len, 1))\n",
    "            aux = np.tile([log_abs_min, log_abs_max, log_abs_mean, log_std], (self.seq_len, 1))\n",
    "            X = np.concatenate((X, aux), axis=-1)\n",
    "\n",
    "        # 6 convert X and mask from float64 to float32\n",
    "        X = X.astype(np.float32)\n",
    "        mask = mask.astype(np.float32)\n",
    "\n",
    "        return X, mask\n",
    "\n",
    "    def readLRSFits(self, filename):\n",
    "        \"\"\"\n",
    "        Read LAMOST fits file\n",
    "          adapted from https://github.com/fandongwei/pylamost\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        filename: str\n",
    "          name of the fits file\n",
    "        z_corr: bool.\n",
    "          if True, correct for measured radial velocity of star\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        spec: numpy array\n",
    "          wavelength, flux, inverse variance\n",
    "        \"\"\"\n",
    "\n",
    "        hdulist = fits.open(filename)\n",
    "        len_list = len(hdulist)\n",
    "\n",
    "        if len_list == 1:\n",
    "            head = hdulist[0].header\n",
    "            scidata = hdulist[0].data\n",
    "            coeff0 = head['COEFF0']\n",
    "            coeff1 = head['COEFF1']\n",
    "            pixel_num = head['NAXIS1']\n",
    "            specflux = scidata[0,]\n",
    "            ivar = scidata[1,]\n",
    "            wavelength = np.linspace(0, pixel_num - 1, pixel_num)\n",
    "            wavelength = np.power(10, (coeff0 + wavelength * coeff1))\n",
    "            hdulist.close()\n",
    "        elif len_list == 2:\n",
    "            head = hdulist[0].header\n",
    "            scidata = hdulist[1].data\n",
    "            wavelength = scidata[0][2]\n",
    "            ivar = scidata[0][1]\n",
    "            specflux = scidata[0][0]\n",
    "        else:\n",
    "            raise ValueError(f'Wrong number of fits files. {len_list} should be 1 or 2')\n",
    "\n",
    "        if self.z_corr:\n",
    "            try:\n",
    "                # correct for radial velocity of star\n",
    "                redshift = head['Z']\n",
    "            except Exception as e:\n",
    "                print(e, 'Setting redshift to zero')\n",
    "                redshift = 0.0\n",
    "\n",
    "            wavelength = wavelength - redshift * wavelength\n",
    "\n",
    "        return np.vstack((wavelength, specflux, ivar)).T\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get_el(self, idx):\n",
    "        el = self.df.iloc[idx]\n",
    "        label = self.target2id[el['variable_type']]\n",
    "\n",
    "        photometry = self.get_vlc(el['asassn_name'])\n",
    "        photometry, photometry_mask = self.preprocess_lc(photometry, el['period'], band='v')\n",
    "\n",
    "        spectra = self.readLRSFits(os.path.join(self.lamost_spec_dir, el['spec_filename']))\n",
    "        wavelengths, fluxes = spectra[:, 0], spectra[:, 1]\n",
    "        fluxes = np.interp(np.arange(3850, 9000, 2), wavelengths, fluxes)\n",
    "        fluxes = (fluxes - fluxes.mean()) / fluxes.std()\n",
    "        fluxes = fluxes.reshape(1, -1).astype(np.float32)\n",
    "\n",
    "        metadata = el[self.metadata_cols].values.astype(np.float32)\n",
    "\n",
    "        return photometry, photometry_mask, fluxes, metadata, label\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        photometry, photometry_mask, fluxes, metadata, label = self.get_el(idx)\n",
    "                \n",
    "        if np.random.rand() < 0.5:\n",
    "            # negatibe sample\n",
    "            idx2 = self.df[self.df['variable_type'] != self.id2target[label]].sample(n=1).index[0]\n",
    "            y = 0\n",
    "        else:\n",
    "            # positive sample\n",
    "            idx2 = self.df[(self.df.index != idx) & (self.df['variable_type'] == self.id2target[label])].sample(n=1).index[0]\n",
    "            y = 1\n",
    "\n",
    "        photometry2, photometry_mask2, fluxes2, metadata2, label2 = self.get_el(idx2)\n",
    "    \n",
    "        return (photometry, photometry_mask, fluxes, metadata), (photometry2, photometry_mask2, fluxes2, metadata2), y"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf56dfc1-dff1-4b71-83d7-5460462fac84",
   "metadata": {},
   "source": [
    "def get_config(random_seed):\n",
    "    config = {\n",
    "        'project': 'multimodal-contrastive',\n",
    "        'random_seed': random_seed,\n",
    "        'use_wandb': True,\n",
    "        'save_weights': True,\n",
    "        'weights_path': f'/home/mariia/AstroML/weights/{datetime.now().strftime(\"%Y-%m-%d-%H-%M\")}',\n",
    "        'use_pretrain': None,\n",
    "\n",
    "        # Data General\n",
    "        'data_root': '/home/mariia/AstroML/data/asassn/',\n",
    "        'file': 'spectra_v_merged.csv',\n",
    "        'classes': None,\n",
    "        'min_samples': 200,\n",
    "        'max_samples': None,\n",
    "\n",
    "        # Photometry\n",
    "        'v_zip': 'asassnvarlc_vband_complete.zip',\n",
    "        'v_prefix': 'vardb_files',\n",
    "        'seq_len': 200,\n",
    "        'phased': True,\n",
    "        'clip': False,\n",
    "        'aux': True,\n",
    "\n",
    "        # Spectra\n",
    "        'lamost_spec_dir': 'Spectra/v2',\n",
    "        'spectra_v_file': 'spectra_v_merged.csv',\n",
    "        'z_corr': False,\n",
    "\n",
    "        # Photometry Model\n",
    "        'p_encoder_layers': 8,\n",
    "        'p_d_model': 128,\n",
    "        'p_dropout': 0.1,\n",
    "        'p_feature_size': 3,\n",
    "        'p_n_heads': 4,\n",
    "        'p_d_ff': 512,\n",
    "\n",
    "        # Spectra Model\n",
    "        's_hidden_dim': 512,\n",
    "        's_dropout': 0.2,\n",
    "\n",
    "        # Metadata Model\n",
    "        'm_hidden_dim': 512,\n",
    "        'm_dropout': 0.5,\n",
    "\n",
    "        # MultiModal Model\n",
    "        'hidden_dim': 256,\n",
    "\n",
    "        # Training\n",
    "        'batch_size': 128,\n",
    "        'lr': 1e-3,\n",
    "        'weight_decay': 0.01,\n",
    "        'epochs': 50,\n",
    "        'optimizer': 'AdamW',\n",
    "        'early_stopping_patience': 10,\n",
    "\n",
    "        # Learning Rate Scheduler\n",
    "        'factor': 0.3,\n",
    "        'patience': 5,\n",
    "    }\n",
    "\n",
    "    return config"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e391653-b569-4d69-b38d-757e3c201179",
   "metadata": {},
   "source": [
    "class Informer(nn.Module):\n",
    "    \"\"\"\n",
    "    Informer with Propspare attention in O(LlogL) complexity\n",
    "    Paper link: https://ojs.aaai.org/index.php/AAAI/article/view/17325/17132\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_in=144, d_model=512, dropout=0.1, factor=1, output_attention=False, n_heads=8, d_ff=2048,\n",
    "                 activation='gelu', e_layers=2):\n",
    "        super(Informer, self).__init__()\n",
    "\n",
    "        self.enc_embedding = DataEmbedding(enc_in, d_model)\n",
    "\n",
    "        attn_layers = [\n",
    "            EncoderLayer(\n",
    "                AttentionLayer(\n",
    "                    ProbAttention(False, factor, attention_dropout=dropout, output_attention=output_attention),\n",
    "                    d_model,\n",
    "                    n_heads\n",
    "                ),\n",
    "                d_model,\n",
    "                d_ff,\n",
    "                dropout=dropout,\n",
    "                activation=activation\n",
    "            ) for _ in range(e_layers)\n",
    "        ]\n",
    "        self.encoder = Encoder(attn_layers, norm_layer=torch.nn.LayerNorm(d_model))\n",
    "\n",
    "        self.act = F.gelu\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc):\n",
    "        # enc\n",
    "        enc_out = self.enc_embedding(x_enc)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=None)\n",
    "\n",
    "        # Output\n",
    "        enc_out = self.dropout(enc_out)\n",
    "        output = enc_out * x_mark_enc.unsqueeze(-1)  # zero-out padding embeddings\n",
    "        output = output.reshape(output.shape[0], -1)  # (batch_size, seq_length * d_model)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class GalSpecNet(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(GalSpecNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(1, 64, kernel_size=3), nn.ReLU())\n",
    "        self.mp1 = nn.MaxPool1d(kernel_size=4)\n",
    "        self.conv2 = nn.Sequential(nn.Conv1d(64, 64, kernel_size=3), nn.ReLU())\n",
    "        self.mp2 = nn.MaxPool1d(kernel_size=4)\n",
    "        self.conv3 = nn.Sequential(nn.Conv1d(64, 32, kernel_size=3), nn.ReLU())\n",
    "        self.mp3 = nn.MaxPool1d(kernel_size=4)\n",
    "        self.conv4 = nn.Sequential(nn.Conv1d(32, 32, kernel_size=3), nn.ReLU())\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.mp1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.mp2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.mp3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, input_dim=36, hidden_dim=128, dropout=0.5):\n",
    "        super(MetaModel, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ModelV0(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ModelV0, self).__init__()\n",
    "\n",
    "        self.photometry_encoder = Informer(\n",
    "            enc_in=config['p_feature_size'], d_model=config['p_d_model'], dropout=config['p_dropout'], factor=1,\n",
    "            output_attention=False, n_heads=config['p_n_heads'], d_ff=config['p_d_ff'],\n",
    "            activation='gelu', e_layers=config['p_encoder_layers']\n",
    "        )\n",
    "        self.spectra_encoder = GalSpecNet(dropout=config['s_dropout'])\n",
    "        self.metadata_encoder = MetaModel(hidden_dim=config['s_hidden_dim'], dropout=config['s_dropout'])\n",
    "\n",
    "        self.photometry_proj = nn.Linear(config['seq_len'] * config['p_d_model'], config['hidden_dim'])\n",
    "        self.spectra_proj = nn.Linear(1184, config['hidden_dim'])\n",
    "        self.metadata_proj = nn.Linear(config['m_hidden_dim'], config['hidden_dim'])\n",
    "\n",
    "        self.cos = nn.CosineSimilarity(dim=1)\n",
    "\n",
    "    def forward(self, el1, el2):\n",
    "        photometry1, photometry_mask1, spectra1, metadata1 = el1\n",
    "        photometry2, photometry_mask2, spectra2, metadata2 = el2\n",
    "\n",
    "        p_emb1 = self.photometry_proj(self.photometry_encoder(photometry1, photometry_mask1))\n",
    "        p_emb2 = self.photometry_proj(self.photometry_encoder(photometry2, photometry_mask2))\n",
    "        s_emb1 = self.spectra_proj(self.spectra_encoder(spectra1))\n",
    "        s_emb2 = self.spectra_proj(self.spectra_encoder(spectra2))\n",
    "        m_emb1 = self.metadata_proj(self.metadata_encoder(metadata1))\n",
    "        m_emb2 = self.metadata_proj(self.metadata_encoder(metadata2))\n",
    "\n",
    "        p_similarity = self.cos(p_emb1, p_emb2)\n",
    "        s_similarity = self.cos(s_emb1, s_emb2)\n",
    "        m_similarity = self.cos(m_emb1, m_emb2)\n",
    "\n",
    "        return p_similarity, s_similarity, m_similarity"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "794aaf61-fdec-468e-92eb-32e4677cfeab",
   "metadata": {},
   "source": [
    "train_dataset = VPSMDataset(split='train')\n",
    "val_dataset = VPSMDataset(split='val')\n",
    "test_dataset = VPSMDataset(split='test')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bbcddec8-4826-44e3-b088-913dd7a4bdbf",
   "metadata": {},
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e1666acc-4f04-4082-b52d-806f197a1ce6",
   "metadata": {},
   "source": [
    "train_dataset[0][0][3][2]   # period"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "db46dc0d-4cbc-437d-9743-1ec5930d648f",
   "metadata": {},
   "source": [
    "train_dataset.df.iloc[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d2b4ae10-99ef-48bf-97e2-23e40af61b97",
   "metadata": {},
   "source": [
    "(photometry, photometry_mask, fluxes, metadata), (photometry2, photometry_mask2, fluxes2, metadata2), y = next(iter(train_dataloader))\n",
    "y.shape, photometry.shape, photometry_mask.shape, fluxes.shape, metadata.shape, photometry2.shape, photometry_mask2.shape, fluxes2.shape, metadata2.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0508c4ee-4e18-4e49-b78d-a99a1fa32696",
   "metadata": {},
   "source": [
    "config = get_config(42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3f3c77f1-f563-4ffd-b85e-c38f23fdce1f",
   "metadata": {},
   "source": [
    "model = ModelV0(config)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c212c9e-43b5-4620-87e5-53548a704155",
   "metadata": {},
   "source": [
    "el1, el2, y = next(iter(train_dataloader))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8befbd51-1f67-4acc-b33c-323ef06eeaa5",
   "metadata": {},
   "source": [
    "p_similarity, s_similarity, m_similarity = model(el1, el2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e3a8c281-1cf0-4110-aaa4-68777575689a",
   "metadata": {},
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a9f313b8-817f-45e5-bbb6-daf5b7566c88",
   "metadata": {},
   "source": [
    "p_similarity.dtype, y.dtype"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67732307-8d19-45bc-8942-446f06e5459a",
   "metadata": {},
   "source": [
    "y = y.to(torch.float32)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9be0d34a-df5b-4845-a26f-f5b85ea22268",
   "metadata": {},
   "source": [
    "criterion(p_similarity, y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a05787c8-cea3-4396-9b30-34749e03a515",
   "metadata": {},
   "source": [
    "criterion(s_similarity, y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f3e2d64-2d06-466f-9d66-37eea519a338",
   "metadata": {},
   "source": [
    "criterion(m_similarity, y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "75f35d93-fb87-409e-b349-d009b3762b7f",
   "metadata": {},
   "source": [
    "criterion(p_similarity, y) + criterion(s_similarity, y) + criterion(m_similarity, y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e99afde5-3915-4ad4-b2f8-2a2a07cccea2",
   "metadata": {},
   "source": [
    "el0 = train_dataset[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "81151530-1cd7-4eab-83ba-20928da81b39",
   "metadata": {},
   "source": [
    "label0 = el0[-1]\n",
    "label0"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "81414254-786d-4339-85ee-ad4823769771",
   "metadata": {},
   "source": [
    "train_dataset.id2target[label0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c982b3b3-6dbd-4fb5-8eb6-ba9a18a9e973",
   "metadata": {},
   "source": [
    "train_dataset.df[train_dataset.df['variable_type'] != train_dataset.id2target[label0]].sample(n=1).index[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c45e0d5-49a6-48e3-8087-cfb48472f69f",
   "metadata": {},
   "source": [
    "photometry, photometry_mask, fluxes, metadata, label = train_dataset[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "47d07f9f-c074-44f4-8237-2390ffaee7b3",
   "metadata": {},
   "source": [
    "photometry.shape, photometry_mask.shape, fluxes.shape, metadata.shape, label"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "567088ed-41dd-45d2-a14d-b30cb6dac797",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f6d78-ddac-4480-89b7-a1f3674beb86",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3fe7ce-5b60-4db5-9529-806908a452d4",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062809a-7135-4ed0-be2b-aa45ea460838",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64546131-48df-45f3-83ec-b864256e5e50",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be30bf7-e81b-45c2-91e7-c98921f64709",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34379ca-8502-49be-b693-9f650ace6b70",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f7a5cb-a1c7-4e40-b5fb-0e7ae2588b9d",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f61e41-8d58-4744-b5dd-c076f957df6c",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a821a20-a372-46e0-a48a-7b157031c038",
   "metadata": {},
   "source": [
    "file = '/home/mariia/AstroML/data/asassn/spectra_v_merged.csv'\n",
    "df = pd.read_csv(file)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b392625-f309-450a-a5e5-dac85bf1c187",
   "metadata": {},
   "source": [
    "df = df[METADATA_COLS + ['edr3_source_id', 'variable_type', 'spec_filename', 'asassn_name']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba16d44c-a917-4663-a283-37233b9c020e",
   "metadata": {},
   "source": [
    "df = df.dropna(axis=0, how='any')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56d3af54-da7c-4d0e-a9b3-020388797d19",
   "metadata": {},
   "source": [
    "df.columns"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bcb667f-d31e-4055-8609-b4e1affe7252",
   "metadata": {},
   "source": [
    "df.drop_duplicates(subset=['edr3_source_id'], keep='last', inplace=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec14f4f0-6af5-4c78-b03c-1d2ca94d0aed",
   "metadata": {},
   "source": [
    "df['edr3_source_id']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d9993-b089-4efc-a923-27f81231208f",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b7529-0937-4016-b148-487d1448ff42",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372ba32-c8f8-4ec5-8587-8655aecfd04b",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1c546-ae91-4071-ad70-ba85d900c6b2",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f8751-2b97-4e95-ae63-1d1a361a95d0",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fed9d9-9da4-44c1-a25c-33c5d0924739",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed4e9d1-a3b8-4b50-88d2-abfae1eb8b36",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a1e056-b3d1-421d-8320-a191eeae9d79",
   "metadata": {},
   "source": [
    "\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
