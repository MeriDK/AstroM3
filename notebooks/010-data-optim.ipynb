{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04edef1-c904-4076-9627-5dcb89da8cfd",
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9b5d09-27b5-415a-abf6-1a6ce535ab34",
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "import os\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import InformerConfig, InformerModel \n",
    "from transformers.models.informer.modeling_informer import InformerEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from scipy import stats\n",
    "import json \n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from util.parallelzipfile import ParallelZipFile as ZipFile\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from core.multimodal.dataset import collate_fn, ASASSNVarStarDataset\n",
    "from core.multimodal.dataset2 import VGDataset\n",
    "from core.multimodal.trainer import ClassificationTrainer\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from models.Informer import Informer\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import Dataset\n",
    "import zipfile"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8353df81-60f3-4bb4-b2cc-5f8d22b4d421",
   "metadata": {},
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "a62437b3-16ed-4594-877f-d9e46cd4b602",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Combine both bands to create 1 lcs dataset. Important: each object (based on EDR3_source_id and edr3_source_id) should be only in train, val or test split!\n",
    "Remove duplicates, filter classes, limit samples, split\n",
    "Dataset should preprocess lcs: fold and then sort values based on time\n",
    "Normalize based on ?\n",
    "Then: pad or trim to context_length and cerate corresponding masks for lcs only\n",
    "\"\"\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f965d09-90f9-4f46-961c-b471253095b0",
   "metadata": {},
   "source": [
    "#paths\n",
    "data_root = '/home/mariia/AstroML/data/asassn'\n",
    "v_file = 'asassn_catalog_full.csv'\n",
    "v_folder = os.path.join(data_root, 'asassnvarlc_vband_complete.zip')\n",
    "g_file = 'asassn_variables_x.csv'\n",
    "g_folder = os.path.join(data_root, 'g_band_lcs-001.tar')\n",
    "\n",
    "# outcome columns\n",
    "target_col = {'g': 'ML_classification', 'v': 'variable_type'}\n",
    "\n",
    "# These are the columns that report the (measured) period of the sources.\n",
    "period_col = {'g': 'Period', 'v': 'period'}\n",
    "\n",
    "# Source names\n",
    "id_col = {'g': 'ID', 'v': 'asassn_name'}\n",
    "\n",
    "# What are the columns in the two datafiles that can be used to uniquely identify a source?\n",
    "merge_key = {'g': 'EDR3_source_id', 'v': 'edr3_source_id'}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00395705-66c2-4a7b-a945-25bd9f20338f",
   "metadata": {},
   "source": [
    "v_df = pd.read_csv(os.path.join(data_root, v_file))\n",
    "g_df = pd.read_csv(os.path.join(data_root, g_file))\n",
    "\n",
    "df1 = v_df[[target_col['v'], period_col['v'], id_col['v'], merge_key['v']]].copy()\n",
    "df1['band'] = 'v'\n",
    "\n",
    "df2 = g_df[[target_col['g'], period_col['g'], id_col['g'], merge_key['g']]].copy()\n",
    "df2['band'] = 'g'\n",
    "\n",
    "df1.columns = ['target', 'period', 'name', 'id', 'band']\n",
    "df2.columns = ['target', 'period', 'name', 'id', 'band']\n",
    "\n",
    "df1 = df1[~df1['id'].isna()]\n",
    "df2 = df2[~df2['id'].isna()]\n",
    "\n",
    "df1 = df1.drop_duplicates(subset='id')\n",
    "df2 = df2.drop_duplicates(subset='id')\n",
    "\n",
    "df1['name'] = df1['name'].apply(lambda x: x.replace(' ', ''))\n",
    "df2['name'] = df2['name'].apply(lambda x: x.replace(' ', '_'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acdbaf16-7e0d-4264-bd1f-73bdf5dd6209",
   "metadata": {},
   "source": [
    "def get_vlc(file_name):\n",
    "    csv = BytesIO()\n",
    "    data_path = f'{v_prefix}/{file_name}.dat'\n",
    "\n",
    "    csv.write(reader_v.read(data_path))\n",
    "    csv.seek(0)\n",
    "\n",
    "    lc = pd.read_csv(csv, sep='\\s+', skiprows=2, names=['HJD', 'MAG', 'MAG_ERR', 'FLUX', 'FLUX_ERR'],\n",
    "                     dtype={'HJD': float, 'MAG': float, 'MAG_ERR': float, 'FLUX': float, 'FLUX_ERR': float})\n",
    "\n",
    "    return lc[['HJD', 'FLUX', 'FLUX_ERR']].values"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c293f509-f4f7-476c-b7c1-f0d712ebd165",
   "metadata": {},
   "source": [
    "v_prefix = 'vardb_files'\n",
    "g_prefix = 'g_band_lcs'\n",
    "reader_v = ZipFile('/home/mariia/AstroML/data/asassn/asassnvarlc_vband_complete.zip')\n",
    "reader_g = ZipFile('/home/mariia/AstroML/data/asassn/g_band_lcs.zip')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92cbe9f1-d04c-40e8-93be-ac1be9a8c88a",
   "metadata": {},
   "source": [
    "res = []\n",
    "\n",
    "for el in tqdm(df1['name']):\n",
    "    if len(get_vlc(el)) == 0:\n",
    "        res.append(el)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af6ee912-02a3-4cf5-ae55-9745d720d0eb",
   "metadata": {},
   "source": [
    "df1 = df1[~df1['name'].isin(res)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "454da3b0-a3f2-4e23-ad75-14cf3c840b35",
   "metadata": {},
   "source": [
    "df1.to_csv(os.path.join(data_root, 'v.csv'), index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e6ce83c-aa6f-4ca0-bd57-eef3a2478b8f",
   "metadata": {},
   "source": [
    "def get_glc(file_name):\n",
    "    lc = pd.read_csv(os.path.join(data_root, g_prefix, f'{file_name}.dat'), sep='\\s+', skiprows=2,\n",
    "                     names=['HJD', 'camera', 'mag', 'mag_err', 'flux', 'flux_err', 'FWHM', 'IMAGE'],\n",
    "                     dtype={'HJD': float, 'camera': 'object', 'mag': 'object', 'mag_err': 'object',\n",
    "                            'flux': float, 'flux_err': float, 'FWHM': 'object', 'IMAGE': 'object'})\n",
    "\n",
    "    return lc[['HJD', 'flux', 'flux_err']].values"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d461c84-81cc-4339-8753-f0275f10e4e9",
   "metadata": {},
   "source": [
    "def get_glc2(file_name):\n",
    "    csv = BytesIO()\n",
    "    data_path = f'{g_prefix}/{file_name}.dat'\n",
    "\n",
    "    csv.write(reader_g.read(data_path))\n",
    "    csv.seek(0)\n",
    "\n",
    "    lc = pd.read_csv(csv, sep='\\s+', skiprows=2, names=['HJD', 'camera', 'mag', 'mag_err', 'flux', 'flux_err', 'FWHM', 'IMAGE'],\n",
    "                     dtype={'HJD': float, 'camera': 'object', 'mag': 'object', 'mag_err': 'object',\n",
    "                            'flux': float, 'flux_err': float, 'FWHM': 'object', 'IMAGE': 'object'})\n",
    "    \n",
    "    return lc[['HJD', 'flux', 'flux_err']].values"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a72f19a-f7ca-4735-a132-ad4a5f0fd9d2",
   "metadata": {},
   "source": [
    "res2 = []\n",
    "\n",
    "for el in tqdm(df2['name']):\n",
    "    if len(get_glc2(el)) == 0:\n",
    "        res2.append(el)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8053210e-62d6-4e92-b2e3-4e70ba82ce3d",
   "metadata": {},
   "source": [
    "len(res2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9be8ce77-e2f3-4c4f-89d6-82098045ad3b",
   "metadata": {},
   "source": [
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "df.to_csv(os.path.join(data_root, 'vg_combined.csv'), index=False)\n",
    "df1.to_csv(os.path.join(data_root, 'v.csv'), index=False)\n",
    "df2.to_csv(os.path.join(data_root, 'g.csv'), index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a14bde-cd5c-4e84-a2b6-682bc3da2370",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3050917-74d3-42ef-88ca-9408fc0cd435",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93ce79cc-5c86-4100-b241-90fdd9d75564",
   "metadata": {},
   "source": [
    "import json\n",
    "import os\n",
    "from io import BytesIO\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from filelock import FileLock\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from util.parallelzipfile import ParallelZipFile as ZipFile\n",
    "from util.preprocess_data import clip_outliers\n",
    "\n",
    "\n",
    "class VGDataset(Dataset):\n",
    "    def __init__(self, data_root, vg_file, v_zip='asassnvarlc_vband_complete.zip', g_tar='g_band_lcs-001.tar',\n",
    "                 v_prefix='vardb_files', g_prefix='g_band_lcs', scales_file='scales.json',\n",
    "                 seq_len=200, split='train', min_samples=None, max_samples=None, classes=None, random_seed=42,\n",
    "                 phased=True, periodic=True, clip_outliers=True, verbose=True):\n",
    "        self.data_root = data_root\n",
    "        self.df = pd.read_csv(os.path.join(data_root, vg_file))\n",
    "        self.reader_v = ZipFile(os.path.join(data_root, v_zip))\n",
    "        # self.reader_g = tarfile.open(os.path.join(data_root, g_tar), 'r')\n",
    "        # print('INIT LEN(self.reader_g.GETNAMES())', len(self.reader_g.getnames()))\n",
    "\n",
    "        self.v_prefix = v_prefix\n",
    "        self.g_prefix = g_prefix\n",
    "        # self.lock = FileLock('g_band.lock')\n",
    "\n",
    "        with open(os.path.join(data_root, scales_file)) as f:\n",
    "            self.scales = json.load(f)\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.split = split\n",
    "        self.min_samples = min_samples\n",
    "        self.max_samples = max_samples\n",
    "        self.classes = classes\n",
    "        self.phased = phased\n",
    "        self.periodic = periodic\n",
    "        self.clip_outliers = clip_outliers\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        self._filter_classes()\n",
    "        self._filter_periodic()\n",
    "        self._limit_samples()\n",
    "        self._shuffle_data()\n",
    "        self._split()\n",
    "\n",
    "        self.id2target = {i: x for i, x in enumerate(self.df['target'].unique())}\n",
    "        self.target2id = {v: k for k, v in self.id2target.items()}\n",
    "        self.num_classes = len(self.id2target)\n",
    "\n",
    "    def _split(self):\n",
    "        unique_ids = self.df['id'].unique()\n",
    "        train_ids, temp_ids = train_test_split(unique_ids, test_size=0.2, random_state=self.random_seed)\n",
    "        val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=self.random_seed)\n",
    "\n",
    "        if self.split == 'train':\n",
    "            self.df = self.df[self.df['id'].isin(train_ids)]\n",
    "        elif self.split == 'val':\n",
    "            self.df = self.df[self.df['id'].isin(val_ids)]\n",
    "        elif self.split == 'test':\n",
    "            self.df = self.df[self.df['id'].isin(test_ids)]\n",
    "        else:\n",
    "            print('Split is not train, val, or test. Keeping the whole dataset')\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'{self.split} split is selected: {len(self.df)} objects left.')\n",
    "\n",
    "    def _filter_classes(self):\n",
    "        if self.classes:\n",
    "            if self.verbose:\n",
    "                print(f'Leaving only classes: {self.classes}... ', end='')\n",
    "\n",
    "            self.df = self.df[self.df['target'].isin(self.classes)]\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'{len(self.df)} objects left.')\n",
    "\n",
    "    def _filter_periodic(self):\n",
    "        if self.periodic:\n",
    "            if self.verbose:\n",
    "                print(f'Removing objects without periods... ', end='')\n",
    "\n",
    "            self.df = self.df[~self.df['period'].isna()]\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'{len(self.df)} objects left.')\n",
    "\n",
    "    def _limit_samples(self):\n",
    "        if self.max_samples or self.min_samples:\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f'Removing objects that have more than {self.max_samples} or less than {self.min_samples} samples... ',\n",
    "                    end='')\n",
    "\n",
    "            value_counts = self.df['target'].value_counts()\n",
    "\n",
    "            if self.min_samples:\n",
    "                classes_to_remove = value_counts[value_counts < self.min_samples].index\n",
    "                self.df = self.df[~self.df['target'].isin(classes_to_remove)]\n",
    "\n",
    "            if self.max_samples:\n",
    "                classes_to_limit = value_counts[value_counts > self.max_samples].index\n",
    "                for class_type in classes_to_limit:\n",
    "                    class_indices = self.df[self.df['target'] == class_type].index\n",
    "                    indices_to_keep = np.random.choice(class_indices, size=self.max_samples, replace=False)\n",
    "                    self.df = self.df.drop(index=set(class_indices) - set(indices_to_keep))\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'{len(self.df)} objects left.')\n",
    "\n",
    "    def _shuffle_data(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    def get_vlc(self, file_name):\n",
    "        csv = BytesIO()\n",
    "        data_path = f'{self.v_prefix}/{file_name}.dat'\n",
    "\n",
    "        csv.write(self.reader_v.read(data_path))\n",
    "        csv.seek(0)\n",
    "\n",
    "        lc = pd.read_csv(csv, sep='\\s+', skiprows=2, names=['HJD', 'MAG', 'MAG_ERR', 'FLUX', 'FLUX_ERR'],\n",
    "                         dtype={'HJD': float, 'MAG': float, 'MAG_ERR': float, 'FLUX': float, 'FLUX_ERR': float})\n",
    "\n",
    "        return lc[['HJD', 'FLUX', 'FLUX_ERR']].values\n",
    "\n",
    "    def get_glc(self, file_name):\n",
    "        # print('LEN(self.reader_g.GETNAMES())', len(self.reader_g.getnames()))\n",
    "\n",
    "        # f = self.reader_g.getmember(f'{self.g_prefix}/{file_name}.dat')\n",
    "\n",
    "        # with self.lock:\n",
    "        #     lc = pd.read_csv(self.reader_g.extractfile(f), sep='\\s+', skiprows=2,\n",
    "        #                      names=['HJD', 'camera', 'mag', 'mag_err', 'flux', 'flux_err', 'FWHM', 'IMAGE'],\n",
    "        #                      dtype={'HJD': float, 'camera': 'object', 'mag': 'object', 'mag_err': 'object',\n",
    "        #                             'flux': float, 'flux_err': float, 'FWHM': 'object', 'IMAGE': 'object'})\n",
    "\n",
    "        lc = pd.read_csv(os.path.join(self.data_root, self.g_prefix, f'{file_name}.dat'), sep='\\s+', skiprows=2,\n",
    "                         names=['HJD', 'camera', 'mag', 'mag_err', 'flux', 'flux_err', 'FWHM', 'IMAGE'],\n",
    "                         dtype={'HJD': float, 'camera': 'object', 'mag': 'object', 'mag_err': 'object',\n",
    "                                'flux': float, 'flux_err': float, 'FWHM': 'object', 'IMAGE': 'object'})\n",
    "\n",
    "        return lc[['HJD', 'flux', 'flux_err']].values\n",
    "\n",
    "    def preprocess(self, X, period, band):\n",
    "        # phase\n",
    "        if self.phased:\n",
    "            X = np.vstack(((X[:, 0] % period) / period, X[:, 1], X[:, 2])).T\n",
    "\n",
    "        # sort based on HJD\n",
    "        sorted_indices = np.argsort(X[:, 0])\n",
    "        X = X[sorted_indices]\n",
    "\n",
    "        t, y, y_err = X[:, 0], X[:, 1], X[:, 2]\n",
    "        if self.clip_outliers and len(t) > 20:\n",
    "            t, y, y_err, _, _, _, _, _ = clip_outliers(t, y, y_err, measurements_in_flux_units=True,\n",
    "                                                       initial_clip=(20, 5), clean_only=True)\n",
    "        X = np.vstack((t, y, y_err)).T\n",
    "\n",
    "        # normalize\n",
    "        # mean, std = self.scales[band]['mean'], self.scales[band]['std']\n",
    "        mean = X[:, 1].mean()\n",
    "        mad = stats.median_abs_deviation(X[:, 1])\n",
    "        X[:, 1] = (X[:, 1] - mean) / mad\n",
    "        X[:, 2] = X[:, 2] / mad\n",
    "\n",
    "        # trim/pad and create mask\n",
    "        mask = np.ones(self.seq_len)\n",
    "\n",
    "        if X.shape[0] > self.seq_len:\n",
    "            X = X[:self.seq_len, :]\n",
    "        else:\n",
    "            mask[X.shape[0]:] = 0\n",
    "            X = np.pad(X, ((0, self.seq_len - X.shape[0]), (0, 0)), 'constant', constant_values=(0,))\n",
    "\n",
    "        # convert X and mask from float64 to float32\n",
    "        X = X.astype(np.float32)\n",
    "        mask = mask.astype(np.float32)\n",
    "\n",
    "        return X[:, 1:], mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        el = self.df.iloc[idx]\n",
    "\n",
    "        X = self.get_vlc(el['name']) if el['band'] == 'v' else self.get_glc(el['name'])\n",
    "        X, mask = self.preprocess(X, el['period'], el['band'])\n",
    "        y = self.target2id[el['target']]\n",
    "\n",
    "        return X, mask, y"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86369ca3-edcf-45cd-927b-a90f7597a100",
   "metadata": {},
   "source": [
    "data_root = '/home/mariia/AstroML/data/asassn'\n",
    "classes = ['CWA', 'CWB', 'DCEP', 'DCEPS', 'DSCT', 'EA', 'EB', 'EW', 'HADS', 'M', 'ROT', 'RRAB', 'RRC', 'RRD', 'RVA', 'SR']\n",
    "train_dataset = VGDataset(data_root, 'v.csv', split='train', seq_len=200, max_samples=20000, phased=True, periodic=True, classes=classes)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75095276-f3fd-45a2-8e0a-cf23b0452eda",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows and 5 columns to plot 10 images\n",
    "n = 30\n",
    "\n",
    "for i in range(n, n + 10):\n",
    "    ax = axs[(i - n) // 5, i % 5]  # Determine the position of the subplot\n",
    "    X, mask, y = train_dataset[i]\n",
    "    ax.plot(X[:, 0])  # Plot the second column of the i-th tensor\n",
    "    ax.set_title(f'Plot {i+1} Label {train_dataset.id2target[y]}')  # Optional: Set a title for each subplot\n",
    "    ax.set_xlabel('Index')  # Optional: Set x-axis label\n",
    "    ax.set_ylabel('Value')  # Optional: Set y-axis label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38b3d4b3-1d71-4f81-b671-73336a077f1a",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows and 5 columns to plot 10 images\n",
    "n = 30\n",
    "\n",
    "for i in range(n, n + 10):\n",
    "    ax = axs[(i - n) // 5, i % 5]  # Determine the position of the subplot\n",
    "    X, mask, y = train_dataset[i]\n",
    "    ax.plot(X[:, 1])  # Plot the second column of the i-th tensor\n",
    "    ax.set_title(f'Plot {i+1} Label {train_dataset.id2target[y]}')  # Optional: Set a title for each subplot\n",
    "    ax.set_xlabel('Index')  # Optional: Set x-axis label\n",
    "    ax.set_ylabel('Value')  # Optional: Set y-axis label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cced5e3d-eb6c-4f49-af8d-9f3478548011",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e5cde-dced-4d92-8bda-40713ad4a2b1",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5725e6-e241-4e79-9bbb-c6a086504beb",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7475792b-d97e-4bd9-bb96-288de8099dcd",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2262739a-37e3-4d46-8219-97b76994852c",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0c6cf4-5532-479f-a6b0-1c27fd344302",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86282cf9-5ea5-4c79-8cae-d91ab27bae22",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db89b1f-4bcc-46fc-89a5-79f8e84bec7d",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cdf045-b2e6-4a73-bc77-248bd66e95d4",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6157199f-5072-407a-b268-bd2c5a6d2436",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "d0b7bfcd-f84a-459e-bacc-7872e4946bb1",
   "metadata": {},
   "source": [
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ebf1f-1874-4e11-8fd1-2227b6044e74",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d194cf-f230-4789-a258-02c55da9f8aa",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef80a893-34d5-433a-97bb-585f39efec94",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b41b1-0b36-4bda-812a-c0904817dd2c",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a72103-870f-4091-bf88-907030f3b47c",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e32d9d9e-6153-459a-98ed-211dcb4cc60c",
   "metadata": {},
   "source": [
    "with open(os.path.join(data_root, 'g_band_lcs/ASASSN-V_J081525.82+840659.7.dat'), 'r') as file:\n",
    "    content = file.read()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "7a5554a8-1bb3-46ed-a87b-113d25154d51",
   "metadata": {},
   "source": [
    "data_path = os.path.join(data_root, 'vardb_files/ASASSN-VJ171434.75+301729.6.dat')\n",
    "lc1 = pd.read_csv(data_path, delim_whitespace=True, skiprows=2, names=['HJD', 'MAG', 'MAG_ERR', 'FLUX', 'FLUX_ERR'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "81dd8f62-7fc7-4eab-912b-e1925144857b",
   "metadata": {},
   "source": [
    "data_path = os.path.join(data_root, 'g_band_lcs/ASASSN-V_J081525.82+840659.7.dat')\n",
    "lc2 = pd.read_csv(data_path, delim_whitespace=True, skiprows=2, \n",
    "                  names=['HJD', 'camera', 'mag', 'mag_err', 'flux', 'flux_err', 'FWHM', 'IMAGE'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "9a171e65-10c7-4cc2-a0f8-618f53d01aa5",
   "metadata": {},
   "source": [
    "lc2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "a7cea092-a042-4c7b-a659-8e52a2988b96",
   "metadata": {},
   "source": [
    "df[df['period'].isna()]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "56c05d5e-a434-4cfa-9f8b-a9a189c67159",
   "metadata": {},
   "source": [
    "lcs_v = ZipFile(v_folder)\n",
    "lcs_g = tarfile.open(g_folder, 'r')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "fe4a98eb-cade-4f0e-95d7-4507e00c0fe4",
   "metadata": {},
   "source": [
    "df.iloc[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "c9ac1418-1731-4d81-b4a1-c16475b878d4",
   "metadata": {},
   "source": [
    "csv = BytesIO()\n",
    "data_path = 'vardb_files/ASASSN-VJ171434.75+301729.6.dat'\n",
    "\n",
    "csv.write(lcs_v.read(data_path))\n",
    "csv.seek(0)\n",
    "\n",
    "lc1 = pd.read_csv(csv, delim_whitespace=True, skiprows=2, names=['HJD', 'MAG', 'MAG_ERR', 'FLUX', 'FLUX_ERR'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "14dbe08c-5a61-4153-8c18-a4cab0f38602",
   "metadata": {},
   "source": [
    "lc = lc1[['HJD', 'FLUX', 'FLUX_ERR']].values"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "e4e786f8-67de-409f-b64e-45f6707b78e9",
   "metadata": {},
   "source": [
    "plt.plot(lc[:, 0], lc[:, 1], '.')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "8979d4f9-c13c-403d-b3c6-4f7dd002d54b",
   "metadata": {},
   "source": [
    "t, y, dy = lc[:, 0], lc[:, 1], lc[:, 2]\n",
    "P_best = df.iloc[0]['period']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "afb505f9-5f9e-4e3f-8491-9f6de1a48ab3",
   "metadata": {},
   "source": [
    "res = np.vstack(((t % P_best) / P_best, y, dy)).T"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "11d7549f-9fa9-4335-bcfb-3e9b4fbc56d2",
   "metadata": {},
   "source": [
    "plt.plot(res[:, 0], res[:, 1], '.')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "9c833260-925e-4d2d-ab04-93ff8786f30f",
   "metadata": {},
   "source": [
    "np.vstack(((t % P_best) / P_best, y, dy)).shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "ea46e02a-f5d3-48d9-bcf8-ca68e54be5f9",
   "metadata": {},
   "source": [
    "sorted_indices = np.argsort(lc[:, 0])\n",
    "sorted_lc = lc[sorted_indices]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "8e193d8c-385d-41cd-986a-9f92b506b6c9",
   "metadata": {},
   "source": [
    "plt.plot(sorted_lc[:, 0], sorted_lc[:, 1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "70178b30-753d-46e6-9436-3105a43b5818",
   "metadata": {},
   "source": [
    "sorted_indices = np.argsort(res[:, 0])\n",
    "sorted_res = res[sorted_indices]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "8483814c-c81e-42d2-9c07-4a1cff0059e7",
   "metadata": {},
   "source": [
    "plt.plot(sorted_res[:, 0], sorted_res[:, 1])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f65f01-6439-4580-8839-864816af8317",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be3bad-dda9-4f89-bc97-84f1d5e7f3b7",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c120247-aa29-4b80-b176-4e3b18e83b24",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0d436-d256-4db1-ab0c-ebc14527b819",
   "metadata": {},
   "source": [
    "for file_name in tqdm(os.listdir(os.path.join(data_root, 'vardb_files'))):\n",
    "    csv = BytesIO()\n",
    "    data_path = f'vardb_files/{file_name}'\n",
    "    \n",
    "    csv.write(lcs_v.read(data_path))\n",
    "    csv.seek(0)\n",
    "    \n",
    "    lc1 = pd.read_csv(csv, delim_whitespace=True, skiprows=2, names=['HJD', 'MAG', 'MAG_ERR', 'FLUX', 'FLUX_ERR'],\n",
    "                      dtype={'HJD': float, 'MAG': float, 'MAG_ERR': float, 'FLUX': float, 'FLUX_ERR': float})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "2a582469-7cab-4d4b-80f8-7b8863691ae5",
   "metadata": {},
   "source": [
    "lock = FileLock('g_band_zip.lock')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "a0aae1f7-882a-42e4-9943-fe6f45a31649",
   "metadata": {},
   "source": [
    "lock = FileLock('g_band_zip.lock')\n",
    "f = lcs_g.getmember('g_band_lcs/ASASSN-V_J081525.82+840659.7.dat')\n",
    "\n",
    "with lock:\n",
    "    lc2 = pd.read_csv(lcs_g.extractfile(f), delim_whitespace=True, skiprows=2, names=['HJD', 'camera', 'mag', 'mag_err', 'flux', 'flux_err', 'FWHM', 'IMAGE'])\n",
    "\n",
    "print(lc2[['HJD', 'flux', 'flux_err']].values)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "b84cc699-3c0a-4778-88a0-cb70fe00f478",
   "metadata": {},
   "source": [
    "lock = FileLock('g_band_zip.lock')\n",
    "f = lcs_g.extractfile('g_band_lcs/ASASSN-V_J081525.82+840659.7.dat')\n",
    "\n",
    "with lock:\n",
    "    lc3 = pd.read_csv(f, delim_whitespace=True, skiprows=2, names=['HJD', 'camera', 'mag', 'mag_err', 'flux', 'flux_err', 'FWHM', 'IMAGE'])\n",
    "\n",
    "print(lc3[['HJD', 'flux', 'flux_err']].values)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "6c8eda3b-34ec-4abe-ba92-119f1bda2e75",
   "metadata": {},
   "source": [
    "lock = FileLock('g_band_zip.lock')\n",
    "f = lcs_g.extractfile('g_band_lcs/ASASSN-V_J081525.82+840659.7.dat')\n",
    "lc4 = pd.read_csv(f, delim_whitespace=True, skiprows=2, names=['HJD', 'camera', 'mag', 'mag_err', 'flux', 'flux_err', 'FWHM', 'IMAGE'])\n",
    "\n",
    "print(lc4[['HJD', 'flux', 'flux_err']].values)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "23c5d182-4b78-45a9-b961-2bfb1a5199a7",
   "metadata": {},
   "source": [
    "f1 = lcs_g.extractfile('g_band_lcs/ASASSN-V_J081525.82+840659.7.dat')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "38192081-87a7-4049-bd90-af6344be1e70",
   "metadata": {},
   "source": [
    "with lock:\n",
    "    lc3 = pd.read_csv(f1, delim_whitespace=True, skiprows=2, names=['HJD', 'camera', 'mag', 'mag_err', 'flux', 'flux_err', 'FWHM', 'IMAGE'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "5784dccf-900c-4cf7-a3b1-62b348a17e2b",
   "metadata": {},
   "source": [
    "lc3[['HJD', 'flux', 'flux_err']].values"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "bacd81a8-c318-4790-9012-a0f5446ae94f",
   "metadata": {},
   "source": [
    "with lock:\n",
    "    lc2 = pd.read_csv(lcs_g.extractfile(f), delim_whitespace=True, skiprows=2, names=['HJD', 'camera', 'mag', 'mag_err', 'flux', 'flux_err', 'FWHM', 'IMAGE'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "ca1d46a4-2019-4e64-bcfd-6069cbf2764b",
   "metadata": {},
   "source": [
    "lc2[['HJD', 'flux', 'flux_err']].values"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "6d3b3f72-ed9a-42fd-8e13-ee05cd34dec9",
   "metadata": {},
   "source": [
    "for el in tqdm(df[df['band'] == 'g']['name']):\n",
    "    f = lcs_g.getmember(f'g_band_lcs/{el}.dat')\n",
    "    with lock:\n",
    "        lc2 = pd.read_csv(lcs_g.extractfile(f), delim_whitespace=True, skiprows=2, names=['HJD', 'camera', 'mag', 'mag_err', 'flux', 'flux_err', 'FWHM', 'IMAGE'],\n",
    "                          dtype={'HJD': float, 'camera': 'object', 'mag': 'object', 'mag_err': 'object', 'flux': float, 'flux_err': float, 'FWHM': 'object', 'IMAGE': 'object'})\n",
    "\n",
    "lc2[['HJD', 'flux', 'flux_err']].values"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "3d581a73-0816-4d90-85dd-9ab527f8feef",
   "metadata": {},
   "source": [
    "lc.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "b68a1804-d382-4644-8f5c-1a64c3a18ef2",
   "metadata": {},
   "source": [
    "lc_padded = np.pad(lc, ((0, 27), (0, 0)), 'constant', constant_values=(0,))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "487084c4-50ca-471e-8b65-742a566c8a34",
   "metadata": {},
   "source": [
    "mask = np.ones((200, 3))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "c54c9efc-dec7-4dca-8359-e6cdbe57004b",
   "metadata": {},
   "source": [
    "unique_ids = df['id'].unique()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "dcd8416b-1bab-40b3-b337-0fe3bc37cef7",
   "metadata": {},
   "source": [
    "train_ids, temp_ids = train_test_split(unique_ids, test_size=0.2)\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=0.5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "47670855-8a95-44d7-9950-53254b4340d8",
   "metadata": {},
   "source": [
    "np.intersect1d(train_ids, val_ids)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e291cfcc-f056-4c81-b89d-da6aafca1912",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb9604-faa5-4397-8f91-ec4212d4de61",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de6f36-338e-42f5-ae50-1b23ec4869f2",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937b99a-77f4-46ca-89d9-dc6da5cf066d",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "de61563d-5b31-4461-a288-cf8bfec695af",
   "metadata": {},
   "source": [
    "class VGDataset(Dataset):\n",
    "    def __init__(self, data_root, vg_file, v_zip='asassnvarlc_vband_complete.zip', g_tar='g_band_lcs-001.tar', v_prefix='vardb_files', g_prefix='g_band_lcs', scales_file='scales.json',\n",
    "                 seq_len=200, split='train', min_samples=5000, max_samples=50000, classes=None, random_seed=42, phased=True, periodic=True, verbose=True):\n",
    "        self.data_root = data_root\n",
    "        self.df = pd.read_csv(os.path.join(data_root, vg_file))\n",
    "        self.reader_v = ZipFile(os.path.join(data_root, v_zip))\n",
    "        self.reader_g = tarfile.open(os.path.join(data_root, g_tar), 'r')\n",
    "        self.v_prefix = v_prefix\n",
    "        self.g_prefix = g_prefix\n",
    "        self.lock = FileLock('g_band_zip.lock')\n",
    "\n",
    "        with open(os.path.join(data_root, scales_file)) as f:\n",
    "            self.scales = json.load(f)\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.split = split\n",
    "        self.min_samples = min_samples\n",
    "        self.max_samples = max_samples\n",
    "        self.classes = classes\n",
    "        self.phased = phased\n",
    "        self.periodic = periodic\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        self._filter_classes()\n",
    "        self._filter_periodic()\n",
    "        self._limit_samples()\n",
    "        self._split()\n",
    "\n",
    "        self.id2target = {i: x for i, x in enumerate(self.df['target'].unique())}\n",
    "        self.target2id = {v: k for k, v in self.id2target.items()}\n",
    "\n",
    "    def _split(self):        \n",
    "        unique_ids = self.df['id'].unique()\n",
    "        train_ids, temp_ids = train_test_split(unique_ids, test_size=0.2, random_state=self.random_seed)\n",
    "        val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=self.random_seed)\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            self.df = self.df[self.df['id'].isin(train_ids)]\n",
    "        elif self.split == 'val':\n",
    "            self.df = self.df[self.df['id'].isin(val_ids)]\n",
    "        elif self.split == 'test':\n",
    "            self.df = self.df[self.df['id'].isin(test_ids)]\n",
    "        else:\n",
    "            print('Split is not train, val, or test. Keeping the whole dataset')\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'{self.split} split is selected: {len(self.df)} objects left.')\n",
    "\n",
    "    def _filter_classes(self):\n",
    "        if self.classes:\n",
    "            if self.verbose:\n",
    "                print(f'Leaving only classes: {self.classes}... ', end='')\n",
    "                        \n",
    "            self.df = self.df[self.df['target'].isin(self.classes)]\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'{len(self.df)} objects left.')\n",
    "\n",
    "    def _filter_periodic(self):\n",
    "        if self.periodic:\n",
    "            if self.verbose:\n",
    "                print(f'Removing objects without periods... ', end='')\n",
    "\n",
    "            self.df = self.df[~self.df['period'].isna()]\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'{len(self.df)} objects left.')\n",
    "            \n",
    "    def _limit_samples(self):\n",
    "        if self.max_samples or self.min_samples:\n",
    "            if self.verbose:\n",
    "                print(f'Removing objects that have more than {self.max_samples} or less than {self.min_samples} samples... ', end='')\n",
    "                \n",
    "            value_counts = self.df['target'].value_counts()\n",
    "\n",
    "            if self.min_samples:\n",
    "                classes_to_remove = value_counts[value_counts < self.min_samples].index\n",
    "                self.df = self.df[~self.df['target'].isin(classes_to_remove)]\n",
    "                            \n",
    "            if self.max_samples:\n",
    "                classes_to_limit = value_counts[value_counts > self.max_samples].index\n",
    "                for class_type in classes_to_limit:\n",
    "                    class_indices = self.df[self.df['target'] == class_type].index\n",
    "                    indices_to_keep = np.random.choice(class_indices, size=self.max_samples, replace=False)\n",
    "                    self.df = self.df.drop(index=set(class_indices) - set(indices_to_keep))\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'{len(self.df)} objects left.')\n",
    "\n",
    "    def get_vlc(self, file_name):\n",
    "        csv = BytesIO()\n",
    "        data_path = f'{self.v_prefix}/{file_name}.dat'\n",
    "        \n",
    "        csv.write(self.reader_v.read(data_path))\n",
    "        csv.seek(0)\n",
    "        \n",
    "        lc = pd.read_csv(csv, delim_whitespace=True, skiprows=2, names=['HJD', 'MAG', 'MAG_ERR', 'FLUX', 'FLUX_ERR'],\n",
    "                         dtype={'HJD': float, 'MAG': float, 'MAG_ERR': float, 'FLUX': float, 'FLUX_ERR': float}) \n",
    "        \n",
    "        return lc[['HJD', 'FLUX', 'FLUX_ERR']].values\n",
    "\n",
    "    def get_glc(self, file_name):\n",
    "        f = self.reader_g.getmember(f'{self.g_prefix}/{file_name}.dat')\n",
    "        \n",
    "        with self.lock:\n",
    "            lc = pd.read_csv(self.reader_g.extractfile(f), delim_whitespace=True, skiprows=2, names=['HJD', 'camera', 'mag', 'mag_err', 'flux', 'flux_err', 'FWHM', 'IMAGE'],\n",
    "                             dtype={'HJD': float, 'camera': 'object', 'mag': 'object', 'mag_err': 'object', 'flux': float, 'flux_err': float, 'FWHM': 'object', 'IMAGE': 'object'})\n",
    "\n",
    "        return lc[['HJD', 'flux', 'flux_err']].values\n",
    "        \n",
    "    def preprocess(self, X, period, band):\n",
    "        # phase\n",
    "        if self.phased:\n",
    "            X = np.vstack(((X[:, 0] % period) / period, X[:, 1], X[:, 2])).T\n",
    "\n",
    "        # sort based on HJD\n",
    "        sorted_indices = np.argsort(X[:, 0])\n",
    "        X = X[sorted_indices]\n",
    "\n",
    "        # normalize\n",
    "        mean, std = self.scales[band]['mean'], self.scales[band]['std']\n",
    "        X[:, 1] = (X[:, 1] - mean) / std\n",
    "        X[:, 2] = X[:, 2] / std\n",
    "\n",
    "        # trim/pad and create mask\n",
    "        mask = np.ones((self.seq_len, 3))\n",
    "        \n",
    "        if X.shape[0] > self.seq_len:\n",
    "            X = X[:self.seq_len, :]\n",
    "        else:\n",
    "            mask[X.shape[0]:, :] = 0\n",
    "            X = np.pad(X, ((0, self.seq_len - X.shape[0]), (0, 0)), 'constant', constant_values=(0,))\n",
    "\n",
    "        return X, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        el = self.df.iloc[idx]\n",
    "\n",
    "        X = self.get_vlc(el['name']) if el['band'] == 'v' else self.get_glc(el['name'])\n",
    "        X, mask = self.preprocess(X, el['period'], el['band'])\n",
    "        y = self.target2id[el['target']]\n",
    "\n",
    "        return X, mask, y"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dfe465-d516-4ee5-837d-f9527ea9dd9a",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb194b9-bbb0-40f1-851b-15a3eeb0faef",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "0e5058da-247a-4aa7-b944-b004e7cfae1b",
   "metadata": {},
   "source": [
    "data_root = '/home/mariia/AstroML/data/asassn'\n",
    "vg_file = 'vg_combined.csv'\n",
    "\n",
    "train_dataset = VGDataset(data_root, vg_file, classes=None, split='train')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "f50fd93f-c3ea-4ec9-9c92-14f6b1e75e44",
   "metadata": {},
   "source": [
    "X, mask, y = train_dataset[1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "e172cfe5-0840-443a-8064-9b6f92031a1d",
   "metadata": {},
   "source": [
    "X.shape, mask.shape, y"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "b63b05ce-dc3b-4ba2-8113-a1662ee99e9a",
   "metadata": {},
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=512)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "a80eab52-fcf5-4f97-905b-2f806be75917",
   "metadata": {},
   "source": [
    "for i in tqdm(range(10)):\n",
    "    batch = next(iter(train_dataloader))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "2e3a6a66-34db-4bd6-9caa-f998844f5c77",
   "metadata": {},
   "source": [
    "old_train_dataloader = DataLoader(ds_train, batch_size=512, collate_fn=no_spectra_collate_fn)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "c4732242-0473-4300-885b-37e8b2301583",
   "metadata": {},
   "source": [
    "for i in tqdm(range(10)):\n",
    "    batch = next(iter(old_train_dataloader))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "a8227154-3ecf-40b1-80a0-db4aace6cecf",
   "metadata": {},
   "source": [
    "old_batch = next(iter(old_train_dataloader))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "b6059794-530e-4e77-a810-6a413f87d506",
   "metadata": {},
   "source": [
    "b, m = old_batch"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "480c18d8-9cfb-4d52-9870-dffbb43ba598",
   "metadata": {},
   "source": [
    "X, y = b\n",
    "X_m, y_m = m"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "5e897af8-d151-4cbe-8693-938c3841e485",
   "metadata": {},
   "source": [
    "X.shape, X_m.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "51ef7af0-8620-42e7-8bc6-76d9775c68f5",
   "metadata": {},
   "source": [
    "X, mask, y = next(iter(train_dataloader))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "74c6a5cc-ea62-4322-843d-cf18e3dd89e5",
   "metadata": {},
   "source": [
    "X.shape, mask.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d50a84a-05be-496c-bdad-64552943116f",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c02ce-128e-4e91-bf50-452ec02d4987",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "3fcd6b01-b816-47fc-9965-648827f408c4",
   "metadata": {},
   "source": [
    "res = {i: x for i, x in enumerate(df['target'].unique())}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f9d95-570c-4678-b181-a50139aa020b",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e406104-83f0-4ecb-a126-3a527f51244c",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d8957-f72f-491c-b8c3-f2258a2764bc",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f820f48-e604-433e-9bd6-7efbbf4cdb09",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67ec6f-609a-4793-a9f4-0cc85fdb5659",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "71605754-4bce-446d-8aee-57f06d1059f9",
   "metadata": {},
   "source": [
    "def preprocess_batch(batch, masks):\n",
    "    lcs, classes = batch\n",
    "    lcs_mask, classes_mask = masks\n",
    "\n",
    "    # shape now [128, 1, 3, 759], make [128, 3, 759] \n",
    "    X = lcs[:, 0, :, :]\n",
    "    \n",
    "    # change axises, shape now [128, 3, 759], make [128, 759, 3]\n",
    "    X = X.transpose(1, 2)\n",
    "    \n",
    "    # since mask is the same for time flux and flux err we can make it 2D\n",
    "    mask = lcs_mask[:, 0, 0, :]\n",
    "\n",
    "    # context length 200, crop X and MASK if longer, pad if shorter\n",
    "    if X.shape[1] < context_length:\n",
    "        X_padding = (0, 0, 0, context_length - X.shape[1], 0, 0)\n",
    "        mask_padding = (0, context_length - X.shape[1])\n",
    "        X = F.pad(X, X_padding)\n",
    "        mask = F.pad(mask, mask_padding, value=True)\n",
    "    else:\n",
    "        X = X[:, :context_length, :]\n",
    "        mask = mask[:, :context_length]\n",
    "\n",
    "    # the last dimention is (time, flux, flux_err), sort it based on time\n",
    "    sort_indices = torch.argsort(X[:, :, 0], dim=1)\n",
    "    sorted_X = torch.zeros_like(X)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        sorted_X[i] = X[i, sort_indices[i]]\n",
    "    \n",
    "    # rearange indexes for masks as well\n",
    "    sorted_mask = torch.zeros_like(mask)\n",
    "    \n",
    "    for i in range(mask.shape[0]):\n",
    "        sorted_mask[i] = mask[i, sort_indices[i]]\n",
    "\n",
    "    # mask should be 1 for values that are observed and 0 for values that are missing\n",
    "    sorted_mask = 1 - sorted_mask.int()\n",
    "\n",
    "    # read scales\n",
    "    with open('scales.json', 'r') as f:\n",
    "        scales = json.load(f)\n",
    "        mean, std = scales['v']['mean'], scales['v']['std']\n",
    "\n",
    "    # scale X\n",
    "    sorted_X[:, :, 1] = (sorted_X[:, :, 1] - mean) / std\n",
    "    sorted_X[:, :, 2] = sorted_X[:, :, 2] / std\n",
    "\n",
    "    # reshape classes to be 1D vector and convert from float to int\n",
    "    classes = classes[:, 0]\n",
    "    classes = classes.long()\n",
    "    \n",
    "    return sorted_X, sorted_mask, classes"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "85be425f-dfd2-404d-be12-97fe65d30d0e",
   "metadata": {},
   "source": [
    "datapath = Path('../data/asassn')\n",
    "ds_train = ASASSNVarStarDataset(datapath, mode='train', verbose=True, only_periodic=True, recalc_period=False, \n",
    "                                prime=True, use_bands=['v'], only_sources_with_spectra=False, return_phased=True, \n",
    "                                fill_value=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e7ec488d-4552-4c17-8c0d-f262f098289d",
   "metadata": {},
   "source": [
    "context_length = 200\n",
    "no_spectra_data_keys = ['lcs', 'classes']\n",
    "no_spectra_collate_fn = partial(collate_fn, data_keys=no_spectra_data_keys, fill_value=0)\n",
    "\n",
    "train_dataloader = DataLoader(ds_train, batch_size=16, shuffle=True, num_workers=0, \n",
    "                              collate_fn=no_spectra_collate_fn)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f961a776-9a57-49a2-8eff-60839cd5d0cd",
   "metadata": {},
   "source": [
    "batch, batch_mask = next(iter(train_dataloader))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f20b5e16-a1a7-4023-84cc-6324b7bfa2c8",
   "metadata": {},
   "source": [
    "X, mask, y = preprocess_batch(batch, batch_mask)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2cf5d9-2d59-454c-99e4-725c29645452",
   "metadata": {},
   "source": [
    "0.5 -flux"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d2d6081a-5a4b-4772-8ef7-040faccf0594",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows and 5 columns to plot 10 images\n",
    "\n",
    "for i in range(10):\n",
    "    ax = axs[i // 5, i % 5]  # Determine the position of the subplot\n",
    "    ax.plot(ds_train[i]['lcs'][0][0][:, 1])  # Plot the second column of the i-th tensor\n",
    "    ax.set_title(f'Plot {i+1}')  # Optional: Set a title for each subplot\n",
    "    ax.set_xlabel('Index')  # Optional: Set x-axis label\n",
    "    ax.set_ylabel('Value')  # Optional: Set y-axis label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1c3a0cbe-6e6f-4612-9adf-16b3d39f6838",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows and 5 columns to plot 10 images\n",
    "\n",
    "for i in range(10):\n",
    "    ax = axs[i // 5, i % 5]  # Determine the position of the subplot\n",
    "    ax.plot(ds_train[i]['lcs'][0][0][:, 2])  # Plot the second column of the i-th tensor\n",
    "    ax.set_title(f'Plot {i+1}')  # Optional: Set a title for each subplot\n",
    "    ax.set_xlabel('Index')  # Optional: Set x-axis label\n",
    "    ax.set_ylabel('Valaue')  # Optional: Set y-axis label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ad85c983-052c-4fbe-bd7f-54d06c331b2b",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows and 5 columns to plot 10 images\n",
    "\n",
    "for i in range(10):\n",
    "    ax = axs[i // 5, i % 5]  # Determine the position of the subplot\n",
    "    ax.plot(ds_train[i]['lcs'][0][0][:, 2] / ds_train[i]['lcs'][0][0][:, 1] * 100)  # Plot the second column of the i-th tensor\n",
    "    ax.set_title(f'Plot {i+1}')  # Optional: Set a title for each subplot\n",
    "    ax.set_xlabel('Index')  # Optional: Set x-axis label\n",
    "    ax.set_ylabel('flux / flux_err * 100 %')  # Optional: Set y-axis label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "c7368769-a52c-4c5f-bf4e-0556bac6de9d",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows and 5 columns to plot 10 images\n",
    "\n",
    "for i in range(10):\n",
    "    ax = axs[i // 5, i % 5]  # Determine the position of the subplot\n",
    "    ax.plot(ds_train[i]['lcs'][0][0][:, 1] / ds_train[i]['lcs'][0][0][:, 2])  # Plot the second column of the i-th tensor\n",
    "    ax.set_title(f'Plot {i+1}')  # Optional: Set a title for each subplot\n",
    "    ax.set_xlabel('Index')  # Optional: Set x-axis label\n",
    "    ax.set_ylabel('flux / flux_err')  # Optional: Set y-axis label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109648f1-add4-41c3-bc34-0b4211877858",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89443d53-cd81-4550-a1d7-a77c95e7753a",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1bbd06-e46f-4afd-8dfd-d5c7d4de0a10",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c442f915-8fb6-440f-b823-d6038e9b1cab",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bc0f3c07-964e-41df-9ebd-b2bd5298d8d9",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows and 5 columns to plot 10 images\n",
    "\n",
    "for i in range(10):\n",
    "    ax = axs[i // 5, i % 5]  # Determine the position of the subplot\n",
    "    ax.plot(X[i, :, 2].numpy())  # Plot the second column of the i-th tensor\n",
    "    ax.set_title(f'Plot {i+1}')  # Optional: Set a title for each subplot\n",
    "    ax.set_xlabel('Index')  # Optional: Set x-axis label\n",
    "    ax.set_ylabel('Value')  # Optional: Set y-axis label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b1071c6d-2fa2-49e9-8516-fb7b246c8f01",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows and 5 columns to plot 10 images\n",
    "\n",
    "for i in range(10):\n",
    "    ax = axs[i // 5, i % 5]  # Determine the position of the subplot\n",
    "    ax.plot(X[i, :, 0].numpy(), X[i, :, 1].numpy())  # Plot the second column of the i-th tensor\n",
    "    ax.set_title(f'Plot {i+1}')  # Optional: Set a title for each subplot\n",
    "    ax.set_xlabel('Index')  # Optional: Set x-axis label\n",
    "    ax.set_ylabel('Value')  # Optional: Set y-axis label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9c97cc1f-a546-4d0a-b8b9-ab66f03cb5c3",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows and 5 columns to plot 10 images\n",
    "\n",
    "for i in range(10):\n",
    "    ax = axs[i // 5, i % 5]  # Determine the position of the subplot\n",
    "    ax.plot(X[i, :, 1].numpy())  # Plot the second column of the i-th tensor\n",
    "    ax.set_title(f'Plot {i+1}')  # Optional: Set a title for each subplot\n",
    "    ax.set_xlabel('Index')  # Optional: Set x-axis label\n",
    "    ax.set_ylabel('Value')  # Optional: Set y-axis label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "31d32ba8-2d9d-4f73-b888-e2f4d7a7064a",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows and 5 columns to plot 10 images\n",
    "\n",
    "for i in range(10):\n",
    "    ax = axs[i // 5, i % 5]  # Determine the position of the subplot\n",
    "    ax.plot(mask[i, :].numpy())  # Plot the second column of the i-th tensor\n",
    "    ax.set_title(f'Plot {i+1}')  # Optional: Set a title for each subplot\n",
    "    ax.set_xlabel('Index')  # Optional: Set x-axis label\n",
    "    ax.set_ylabel('Value')  # Optional: Set y-axis label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3c56cef8-ef90-4ed1-b739-9f759f5cb4fe",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows and 5 columns to plot 10 images\n",
    "\n",
    "for i in range(10):\n",
    "    ax = axs[i // 5, i % 5]  # Determine the position of the subplot\n",
    "    ax.plot(batch[0][i, 0, 1, :200].numpy())  # Plot the second column of the i-th tensor\n",
    "    ax.set_title(f'Plot {i+1}')  # Optional: Set a title for each subplot\n",
    "    ax.set_xlabel('Index')  # Optional: Set x-axis label\n",
    "    ax.set_ylabel('Value')  # Optional: Set y-axis label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2401ff19-41fb-44ab-80d8-09e1ae87e9e8",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(15, 6))  # 2 rows and 5 columns to plot 10 images\n",
    "\n",
    "for i in range(10):\n",
    "    ax = axs[i // 5, i % 5]  # Determine the position of the subplot\n",
    "    ax.plot(batch_mask[0][i, 0, 1, :200].numpy())  # Plot the second column of the i-th tensor\n",
    "    ax.set_title(f'Plot {i+1}')  # Optional: Set a title for each subplot\n",
    "    ax.set_xlabel('Index')  # Optional: Set x-axis label\n",
    "    ax.set_ylabel('Value')  # Optional: Set y-axis label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9428f878-a101-46b0-84a5-245eedb4a3e4",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b35485f-d4b5-45b4-9552-523f25fd7043",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0393fb81-23f6-49ae-b757-32b12f195588",
   "metadata": {},
   "source": [
    "for i in tqdm(range(5000)):\n",
    "    el = ds_train[i]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34997a95-0d3d-490b-8008-73f08a05e21b",
   "metadata": {},
   "source": [
    "for i in tqdm(range(10)):\n",
    "    el = next(iter(train_dataloader))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ff534b33-39a7-4afd-acc3-2d48566613e0",
   "metadata": {},
   "source": [
    "batch = [ds_train[i] for i in range(500)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "873cf448-3d19-49f2-afff-54e8b966ebb7",
   "metadata": {},
   "source": [
    "fill_value = 0\n",
    "lcs = [torch.tensor(el['lcs'][0][0].astype(float)).unsqueeze(-1) for el in batch]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "73678776-70fb-4fa7-a3be-fbc42d595a37",
   "metadata": {},
   "source": [
    "res = torch.nn.utils.rnn.pad_sequence(lcs, padding_value=0)\n",
    "res.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c97f51b0-817d-43b2-9073-b7cd76f90d22",
   "metadata": {},
   "source": [
    "# return a tensor like batch_size, lcs/spectrum_number, [nu, flux, fluxerr], value\n",
    "kb = torch.permute(res, (1, 3, 2, 0))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f2cf181e-1726-4303-a40c-d576432c071e",
   "metadata": {},
   "source": [
    "kb.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "09677363-8003-4ec2-9dde-c487d5c0c5ce",
   "metadata": {},
   "source": [
    "b = next(iter(train_dataloader))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a75f3230-6856-4608-a3e1-dbb15f140aba",
   "metadata": {},
   "source": [
    "(x, y), (x_mask, y_mask) = b"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f1da660f-70f7-4336-8bcf-2612fd2d3c25",
   "metadata": {},
   "source": [
    "x.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2228c23-73bf-4c40-a9b9-493fc56c873c",
   "metadata": {},
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "a = torch.ones(25, 300)\n",
    "b = torch.ones(22, 300)\n",
    "c = torch.ones(15, 300)\n",
    "pad_sequence([a, b, c]).size()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45e023-f1dd-4f2d-bac6-eaed795cac54",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f5c349-4c32-4384-bb38-9a088d3a222c",
   "metadata": {},
   "source": [
    "def collate_fn_lcs(batch, fill_value=0):\n",
    "    key_batch = [torch.Tensor(t[k]) for t in batch]\n",
    "    kb = torch.nn.utils.rnn.pad_sequence(\n",
    "        key_batch, batch_first=True, padding_value=fill_value\n",
    "    ).squeeze(1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4383e-bffa-4450-a68a-6d5988e1556f",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142ec27d-85c6-44cf-8aed-5462cfeffe16",
   "metadata": {},
   "source": [
    "def collate_fn(\n",
    "    batch, data_keys=[\"lcs\", \"metadata\", \"spectra\", \"classes\"], fill_value=-9999\n",
    "):\n",
    "    \"\"\"\n",
    "    return a list of tensors with data and masks, for this batch. Makes the\n",
    "    smallest possible tensors given the maximum size of each element in the batch.\n",
    "\n",
    "    See https://pytorch.org/docs/stable/data.html#dataloader-collate-fn\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    batch: list of dicts\n",
    "        each dict is a single object\n",
    "    data_keys: list of strings\n",
    "        which keys to return\n",
    "    fill_value: float\n",
    "        value to use for missing data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    data: list of tensors\n",
    "        the data for each key\n",
    "    masks: list of tensors\n",
    "        the masks for each key\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    masks = []\n",
    "    for k in data_keys:\n",
    "        if k in [\"metadata\", \"classes\"]:\n",
    "            key_batch = [torch.Tensor(t[k]) for t in batch]\n",
    "            kb = torch.nn.utils.rnn.pad_sequence(\n",
    "                key_batch, batch_first=True, padding_value=fill_value\n",
    "            ).squeeze(1)\n",
    "\n",
    "        if k in [\"spectra\", \"lcs\"]:\n",
    "            spec = []\n",
    "            max_spec = 200\n",
    "            book = []\n",
    "            for on, obj in enumerate(batch):\n",
    "                obj_spec = obj[k][0]\n",
    "                s = []\n",
    "                ls = []\n",
    "                for i in range(len(obj_spec)):\n",
    "                    shape = list(obj_spec[i].shape)\n",
    "                    # check for possible empty data vector\n",
    "                    if shape[0] == 0:\n",
    "                        shape[0] += 1\n",
    "                        v = fill_value * torch.ones(shape)\n",
    "                    else:\n",
    "                        v = obj_spec[i]\n",
    "                    s.append(torch.Tensor(v))\n",
    "                    ls.append(s[-1].shape)\n",
    "\n",
    "                if len(obj_spec) > 0:\n",
    "                    last_shape = s[-1].shape\n",
    "                else:\n",
    "                    last_shape = (1, 1)\n",
    "                    book.append((on, last_shape))\n",
    "\n",
    "                # TODO Fix i outside of loop\n",
    "                for j in range(i + 1, max_spec):\n",
    "                    s.append(fill_value * torch.ones(last_shape))\n",
    "                    ls.append(s[-1].shape)\n",
    "\n",
    "                spec.append(\n",
    "                    torch.nn.utils.rnn.pad_sequence(s, padding_value=fill_value)\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                max_shape = tuple(np.max(np.array([s.shape for s in spec]), axis=0))\n",
    "            except Exception as err:\n",
    "                raise err from None\n",
    "\n",
    "            for i, s in enumerate(spec):\n",
    "                if s.shape[0] <= 1:\n",
    "                    book.append((\"here\", i, s.shape))\n",
    "                    spec[i] = fill_value * torch.ones(max_shape)\n",
    "            try:\n",
    "                kb = torch.nn.utils.rnn.pad_sequence(spec, padding_value=fill_value)\n",
    "            except Exception as err:\n",
    "                for i, ss in enumerate(spec):\n",
    "                    print(i, ss.shape, type(ss))\n",
    "                raise err from None\n",
    "\n",
    "            # return a tensor like batch_size, lcs/spectrum_number, [nu, flux, fluxerr],\n",
    "            # value\n",
    "            kb = torch.permute(kb, (1, 2, 3, 0))\n",
    "\n",
    "        # make a mask and fill in.\n",
    "        isnan = torch.isnan(kb)\n",
    "        key_mask = torch.where(kb == fill_value, True, False) | isnan\n",
    "        kb[key_mask] = fill_value\n",
    "        data.append(kb)\n",
    "        masks.append(key_mask)\n",
    "\n",
    "    return data, masks"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a17c9d-f3ff-4289-a892-dae63a2323d6",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1541bea7-cc11-40fd-bfc8-b15e0f793d35",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc6852-3b85-4b2e-b523-0ba4bffda527",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a1ccc-66e9-4a68-868e-ad457966a942",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d8132-85ca-4de5-b019-744f8ab47e9d",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a69b-f321-405e-8703-844e63acc38e",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
