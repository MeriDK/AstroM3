{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "484faa56-fabb-4c8f-8177-5d2ac69e5f77",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import os\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from util.parallelzipfile import ParallelZipFile as ZipFile\n",
    "import csv\n",
    "import json"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f969397-048a-44cb-afac-9b2cea9b7e0e",
   "metadata": {},
   "source": [
    "dataset = load_dataset('MeriDK/AstroM3')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a577224e-a7ca-475f-9a60-d26c1e7abac3",
   "metadata": {},
   "source": [
    "len(dataset['train'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f5ce03-7ca6-459f-81c9-8beda42ebdb5",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b2654-4801-49b4-b82f-2c00ab46c186",
   "metadata": {},
   "source": [
    "# TODO: Add BibTeX citation\n",
    "# Find for instance the citation on arxiv or on the dataset repo/website\n",
    "_CITATION = \"\"\"\\\n",
    "@inproceedings{rizhko2024self,\n",
    "  title={Self-supervised Multimodal Model for Astronomy},\n",
    "  author={Rizhko, Mariia and Bloom, Joshua S},\n",
    "  booktitle={Neurips 2024 Workshop Foundation Models for Science: Progress, Opportunities, and Challenges}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Add description of the dataset here\n",
    "# You can copy an official description\n",
    "_DESCRIPTION = \"\"\"\\\n",
    "This dataset includes 21,440 objects with time-series photometry, spectra, and metadata. \\\n",
    "It is designed for building and testing next-generation multi-modal self-supervised models for astronomy.\n",
    "\"\"\"\n",
    "\n",
    "_HOMEPAGE = \"https://github.com/MeriDK/AstroM3/\"\n",
    "_LICENSE = \"CC BY 4.0\"\n",
    "\n",
    "# resolve ? tree\n",
    "_URL = \"https://huggingface.co/datasets/MeriDK/AstroM3/tree/main\"\n",
    "\n",
    "_URLS = {\n",
    "    \"full\": {\n",
    "        \"train\": \"./splits/spectra_and_v_train.csv\",\n",
    "        \"val\": \"./splits/spectra_and_v_val.csv\",\n",
    "        \"test\": \"./splits/spectra_and_v_test.csv\"\n",
    "    },\n",
    "    \"50\": {},\n",
    "    \"25\": {},\n",
    "    \"10\": {}\n",
    "}\n",
    "\n",
    "\n",
    "# TODO: Name of the dataset usually matches the script name with CamelCase instead of snake_case\n",
    "class AstroM3Dataset(datasets.GeneratorBasedBuilder):\n",
    "    \"\"\"TODO: Short description of my dataset.\"\"\"\n",
    "\n",
    "    VERSION = datasets.Version(\"1.1.0\")\n",
    "\n",
    "    # This is an example of a dataset with multiple configurations.\n",
    "    # If you don't want/need to define several sub-sets in your dataset,\n",
    "    # just remove the BUILDER_CONFIG_CLASS and the BUILDER_CONFIGS attributes.\n",
    "\n",
    "    # If you need to make complex sub-parts in the datasets with configurable options\n",
    "    # You can create your own builder configuration class to store attribute, inheriting from datasets.BuilderConfig\n",
    "    # BUILDER_CONFIG_CLASS = MyBuilderConfig\n",
    "\n",
    "    # You will be able to load one or the other configurations in the following list with\n",
    "    # data = datasets.load_dataset('my_dataset', 'first_domain')\n",
    "    # data = datasets.load_dataset('my_dataset', 'second_domain')\n",
    "    BUILDER_CONFIGS = [\n",
    "        datasets.BuilderConfig(name=\"full\", version=VERSION, description=\"The full dataset\"),\n",
    "        datasets.BuilderConfig(name=\"50\", version=VERSION, description=\"Subsample of the dataset, contains 50% of all data\"),\n",
    "        datasets.BuilderConfig(name=\"25\", version=VERSION, description=\"Subsample of the dataset, contains 25% of all data\"),\n",
    "        datasets.BuilderConfig(name=\"10\", version=VERSION, description=\"Subsample of the dataset, contains 10% of all data\"),\n",
    "    ]\n",
    "\n",
    "    DEFAULT_CONFIG_NAME = \"full\"  # It's not mandatory to have a default configuration. Just use one if it make sense.\n",
    "\n",
    "    def _info(self):\n",
    "        return datasets.DatasetInfo(\n",
    "            # This is the description that will appear on the datasets page.\n",
    "            description=_DESCRIPTION,\n",
    "            # This defines the different columns of the dataset and their types\n",
    "            features=datasets.Features(\n",
    "                {\n",
    "                    \"photometry\": datasets.Array2D(shape=(200, 7), dtype=\"float32\"),\n",
    "                    \"photometry_mask\": datasets.Sequence(datasets.Value(\"float32\")),\n",
    "                    \"spectra\": datasets.Array2D(shape=(200, 7), dtype=\"float32\"),\n",
    "                    \"metadata\": datasets.Sequence(datasets.Value(\"float32\")),\n",
    "                    \"label\": datasets.Value(\"int32\")\n",
    "                }\n",
    "            ),\n",
    "            supervised_keys=None,\n",
    "            # Homepage of the dataset for documentation\n",
    "            homepage=_HOMEPAGE,\n",
    "            # License for the dataset if available\n",
    "            license=_LICENSE,\n",
    "            # Citation for the dataset\n",
    "            citation=_CITATION,\n",
    "        )\n",
    "\n",
    "    def _split_generators(self, dl_manager: DownloadManager):\n",
    "        # TODO: This method is tasked with downloading/extracting the data and defining the splits depending on the configuration\n",
    "        # If several configurations are possible (listed in BUILDER_CONFIGS), the configuration selected by the user is in self.config.name\n",
    "\n",
    "        # dl_manager is a datasets.download.DownloadManager that can be used to download and extract URLS\n",
    "        # It can accept any type or nested list/dict and will give back the same structure with the url replaced with path to local files.\n",
    "        # By default the archives will be extracted and a path to a cached folder where they are extracted is returned instead of the archive\n",
    "        urls = _URLS[self.config.name]\n",
    "        return [\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TRAIN,\n",
    "                # These kwargs will be passed to _generate_examples\n",
    "                gen_kwargs={\n",
    "                    \"split\": \"train\",\n",
    "                },\n",
    "            ),\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.VALIDATION,\n",
    "                # These kwargs will be passed to _generate_examples\n",
    "                gen_kwargs={\n",
    "                    \"split\": \"val\",\n",
    "                },\n",
    "            ),\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TEST,\n",
    "                # These kwargs will be passed to _generate_examples\n",
    "                gen_kwargs={\n",
    "                    \"split\": \"test\"\n",
    "                },\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    # method parameters are unpacked from `gen_kwargs` as given in `_split_generators`\n",
    "    def _generate_examples(self, split):\n",
    "        csv_file = os.path.join(self.config.data_dir, f'spectra_and_v_{split}.csv')\n",
    "        vband_file = os.path.join(self.config.data_dir, 'asassnvarlc_vband_complete.zip')\n",
    "        \n",
    "        df = pd.read_csv(csv_file)\n",
    "        reader_v = ZipFile(vband_file)\n",
    "\n",
    "        # TODO: This method handles input defined in _split_generators to yield (key, example) tuples from the dataset.\n",
    "        # The `key` is for legacy reasons (tfds) and is not important in itself, but must be unique for each example.\n",
    "        for idx, row in df.iterrows():\n",
    "            label = row['target']\n",
    "            photometry, photometry_mask = self.get_vlc(reader_v, row['file_name'])\n",
    "            spectra = self.readLRSFits(f'{split}/{label}/{row[\"spec_filename\"]}')\n",
    "            metadata = np.array(row[self.config[\"meta_cols\"]], dtype=np.float32)\n",
    "\n",
    "            \n",
    "        with open(filepath, encoding=\"utf-8\") as f:\n",
    "            for key, row in enumerate(f):\n",
    "                data = json.loads(row)\n",
    "                if self.config.name == \"first_domain\":\n",
    "                    # Yields examples as (key, example) tuples\n",
    "                    yield key, {\n",
    "                        \"sentence\": data[\"sentence\"],\n",
    "                        \"option1\": data[\"option1\"],\n",
    "                        \"answer\": \"\" if split == \"test\" else data[\"answer\"],\n",
    "                    }\n",
    "                else:\n",
    "                    yield key, {\n",
    "                        \"sentence\": data[\"sentence\"],\n",
    "                        \"option2\": data[\"option2\"],\n",
    "                        \"second_domain_answer\": \"\" if split == \"test\" else data[\"second_domain_answer\"],\n",
    "                    }"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da69f2-1edd-4913-ba83-2d4ce4465a89",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b922220b-6ec7-479c-8015-9b7171791e4e",
   "metadata": {},
   "source": [
    "\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            photometry, photometry_mask = self.get_vlc(reader_v, data_dir, row['file_name'])\n",
    "            spectra = self.readLRSFits(data_dir, row[\"spec_filename\"])\n",
    "            metadata = np.array(row[self.config[\"meta_cols\"]], dtype=np.float32)\n",
    "            label = row[\"label\"]\n",
    "\n",
    "            yield idx, {\n",
    "                \"photometry\": photometry,\n",
    "                \"photometry_mask\": photometry_mask,\n",
    "                \"spectra\": spectra,\n",
    "                \"metadata\": metadata,\n",
    "                \"label\": label\n",
    "            }\n",
    "\n",
    "    def get_vlc(self, reader_v, data_dir, file_name):\n",
    "        csv = BytesIO()\n",
    "        file_name = file_name.replace(' ', '')\n",
    "        data_path = f'vardb_files/{file_name}.dat'\n",
    "\n",
    "        csv.write(reader_v.read(data_path))\n",
    "        csv.seek(0)\n",
    "\n",
    "        lc = pd.read_csv(csv, sep='\\s+', skiprows=2, names=['HJD', 'MAG', 'MAG_ERR', 'FLUX', 'FLUX_ERR'],\n",
    "                         dtype={'HJD': float, 'MAG': float, 'MAG_ERR': float, 'FLUX': float, 'FLUX_ERR': float})\n",
    "\n",
    "        return lc[['HJD', 'FLUX', 'FLUX_ERR']].values\n",
    "\n",
    "    def readLRSFits(self, data_dir, file_name):\n",
    "        path = os.path.join(data_dir, file_name)\n",
    "        hdulist = fits.open(path)\n",
    "        len_list = len(hdulist)\n",
    "\n",
    "        if len_list == 1:\n",
    "            head = hdulist[0].header\n",
    "            scidata = hdulist[0].data\n",
    "            coeff0 = head['COEFF0']\n",
    "            coeff1 = head['COEFF1']\n",
    "            pixel_num = head['NAXIS1']\n",
    "            specflux = scidata[0,]\n",
    "            ivar = scidata[1,]\n",
    "            wavelength = np.linspace(0, pixel_num - 1, pixel_num)\n",
    "            wavelength = np.power(10, (coeff0 + wavelength * coeff1))\n",
    "            hdulist.close()\n",
    "        elif len_list == 2:\n",
    "            head = hdulist[0].header\n",
    "            scidata = hdulist[1].data\n",
    "            wavelength = scidata[0][2]\n",
    "            ivar = scidata[0][1]\n",
    "            specflux = scidata[0][0]\n",
    "        else:\n",
    "            raise ValueError(f'Wrong number of fits files. {len_list} should be 1 or 2')\n",
    "\n",
    "        return np.vstack((wavelength, specflux, ivar)).T\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252803fd-64b5-4aa1-a810-62e6cb78e950",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85218da5-8138-42c7-8282-177a9ec8500c",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c2f51-18a5-4b59-9fce-f412874bdf91",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d350eef-3772-46ca-a693-2b3d9daa8afe",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23920a39-f369-44f1-abf9-0b257d28e8b7",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
