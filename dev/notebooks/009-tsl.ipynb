{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afc7ecec-2920-4b66-8490-58187fac7bd3",
   "metadata": {},
   "source": [
    "!ls \"../../Time-Series-Library\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9b7f20e-1b95-47d2-90ed-58418208dccd",
   "metadata": {},
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../../Time-Series-Library')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0f238500-af53-497e-a83f-7ebad2ebc335",
   "metadata": {},
   "source": [
    "from models.Informer import Model as InformerModel\n",
    "from data_provider.data_loader import UEAloader\n",
    "from data_provider.uea import collate_fn\n",
    "from layers.Transformer_EncDec import Encoder, EncoderLayer, ConvLayer\n",
    "from layers.SelfAttention_Family import ProbAttention, AttentionLayer\n",
    "from layers.Embed import DataEmbedding\n",
    "\n",
    "import math\n",
    "from math import sqrt\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import argparse\n",
    "from sktime.datasets import load_from_tsfile_to_dataframe\n",
    "from torch.utils.data import DataLoader"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c984d68-eb11-402b-af3e-5af2975f96b9",
   "metadata": {},
   "source": [
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b203022d-d2dc-4a6a-8ce7-6328d573bf2d",
   "metadata": {},
   "source": [
    "fix_seed = 42\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "de148fdf-4ae4-48f5-81ca-6c61ade8938e",
   "metadata": {},
   "source": [
    "parser = argparse.ArgumentParser(description='Informer')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--task_name', type=str, required=False, default='long_term_forecast',\n",
    "                    help='task name, options:[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]')\n",
    "parser.add_argument('--is_training', type=int, required=False, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=False, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=False, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Transformer, TimesNet]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=False, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "parser.add_argument('--seasonal_patterns', type=str, default='Monthly', help='subset for M4')\n",
    "parser.add_argument('--inverse', action='store_true', help='inverse output data', default=False)\n",
    "\n",
    "# inputation task\n",
    "parser.add_argument('--mask_rate', type=float, default=0.25, help='mask ratio')\n",
    "\n",
    "# anomaly detection task\n",
    "parser.add_argument('--anomaly_ratio', type=float, default=0.25, help='prior anomaly ratio (%)')\n",
    "\n",
    "# model define\n",
    "parser.add_argument('--expand', type=int, default=2, help='expansion factor for Mamba')\n",
    "parser.add_argument('--d_conv', type=int, default=4, help='conv kernel size for Mamba')\n",
    "parser.add_argument('--top_k', type=int, default=5, help='for TimesBlock')\n",
    "parser.add_argument('--num_kernels', type=int, default=6, help='for Inception')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size')\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--channel_independence', type=int, default=1,\n",
    "                    help='0: channel dependence 1: channel independence for FreTS model')\n",
    "parser.add_argument('--decomp_method', type=str, default='moving_avg',\n",
    "                    help='method of series decompsition, only support moving_avg or dft_decomp')\n",
    "parser.add_argument('--use_norm', type=int, default=1, help='whether to use normalize; True 1 False 0')\n",
    "parser.add_argument('--down_sampling_layers', type=int, default=0, help='num of down sampling layers')\n",
    "parser.add_argument('--down_sampling_window', type=int, default=1, help='down sampling window size')\n",
    "parser.add_argument('--down_sampling_method', type=str, default=None,\n",
    "                    help='down sampling method, only support avg, max, conv')\n",
    "parser.add_argument('--seg_len', type=int, default=48,\n",
    "                    help='the length of segmen-wise iteration of SegRNN')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=1, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='MSE', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "\n",
    "# de-stationary projector params\n",
    "parser.add_argument('--p_hidden_dims', type=int, nargs='+', default=[128, 128],\n",
    "                    help='hidden layer dimensions of projector (List)')\n",
    "parser.add_argument('--p_hidden_layers', type=int, default=2, help='number of hidden layers in projector')\n",
    "\n",
    "# metrics (dtw)\n",
    "parser.add_argument('--use_dtw', type=bool, default=False, \n",
    "                    help='the controller of using dtw metric (dtw is time consuming, not suggested unless necessary)')\n",
    "\n",
    "# Augmentation\n",
    "parser.add_argument('--augmentation_ratio', type=int, default=0, help=\"How many times to augment\")\n",
    "parser.add_argument('--seed', type=int, default=2, help=\"Randomization seed\")\n",
    "parser.add_argument('--jitter', default=False, action=\"store_true\", help=\"Jitter preset augmentation\")\n",
    "parser.add_argument('--scaling', default=False, action=\"store_true\", help=\"Scaling preset augmentation\")\n",
    "parser.add_argument('--permutation', default=False, action=\"store_true\", help=\"Equal Length Permutation preset augmentation\")\n",
    "parser.add_argument('--randompermutation', default=False, action=\"store_true\", help=\"Random Length Permutation preset augmentation\")\n",
    "parser.add_argument('--magwarp', default=False, action=\"store_true\", help=\"Magnitude warp preset augmentation\")\n",
    "parser.add_argument('--timewarp', default=False, action=\"store_true\", help=\"Time warp preset augmentation\")\n",
    "parser.add_argument('--windowslice', default=False, action=\"store_true\", help=\"Window slice preset augmentation\")\n",
    "parser.add_argument('--windowwarp', default=False, action=\"store_true\", help=\"Window warp preset augmentation\")\n",
    "parser.add_argument('--rotation', default=False, action=\"store_true\", help=\"Rotation preset augmentation\")\n",
    "parser.add_argument('--spawner', default=False, action=\"store_true\", help=\"SPAWNER preset augmentation\")\n",
    "parser.add_argument('--dtwwarp', default=False, action=\"store_true\", help=\"DTW warp preset augmentation\")\n",
    "parser.add_argument('--shapedtwwarp', default=False, action=\"store_true\", help=\"Shape DTW warp preset augmentation\")\n",
    "parser.add_argument('--wdba', default=False, action=\"store_true\", help=\"Weighted DBA preset augmentation\")\n",
    "parser.add_argument('--discdtw', default=False, action=\"store_true\", help=\"Discrimitive DTW warp preset augmentation\")\n",
    "parser.add_argument('--discsdtw', default=False, action=\"store_true\", help=\"Discrimitive shapeDTW warp preset augmentation\")\n",
    "parser.add_argument('--extra_tag', type=str, default=\"\", help=\"Anything extra\")\n",
    "\n",
    "args = parser.parse_args([])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "502551f4-13a5-46b8-866e-fe29946b75a5",
   "metadata": {},
   "source": [
    "train_dataset = UEAloader(args, root_path='/home/mariia/Time-Series-Library/dataset/FaceDetection', flag='TRAIN')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3564d896-253c-4fb4-9592-6c077a2831a5",
   "metadata": {},
   "source": [
    "train_dataset[0][0].shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca9db51b-e38d-4d0f-8f94-7b20b3940fc0",
   "metadata": {},
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4,\n",
    "                              collate_fn=lambda x: collate_fn(x, max_len=200))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25372ad8-1b69-4db6-935c-e57f83c85a37",
   "metadata": {},
   "source": [
    "X, y, mask = next(iter(train_dataloader))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fa3ae442-a7db-4172-9f65-65f0a0053d33",
   "metadata": {},
   "source": [
    "X.shape, y.shape, mask.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3287e9a4-e439-41e5-bb9d-f0f9117c2656",
   "metadata": {},
   "source": [
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5169321d-e68e-46a9-b1cb-38228a1f2830",
   "metadata": {},
   "source": [
    "X, mask = X.to(device), mask.to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5b2209ad-a8da-4d51-93d5-e158f2046a15",
   "metadata": {},
   "source": [
    "args.seq_len = 200\n",
    "args.pred_len = 0\n",
    "args.enc_in = train_dataset.feature_df.shape[1]\n",
    "args.num_class = len(train_dataset.class_names)\n",
    "args.task_name = 'classification'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "994b023b-f540-43a5-94b5-e4c900749c72",
   "metadata": {},
   "source": [
    "model = InformerModel(args)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b9bf6876-1d49-4afa-8811-7752d4accd5b",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    output = model(X, mask, None, None)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bc998490-a313-48e4-aad4-153df9103a56",
   "metadata": {},
   "source": [
    "output[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce52f928-1799-4e3e-9ef0-75cd6a88b8bc",
   "metadata": {},
   "source": [
    "args.num_class"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "27e2d6ec-24ee-47a8-b076-3e1056669575",
   "metadata": {},
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(\n",
    "            self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:, :, 3])\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        if x_mark is None:\n",
    "            x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(\n",
    "                x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(x)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e2de6dd3-9b2a-4cea-a78c-5cd6ea1ca845",
   "metadata": {},
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None, tau=None, delta=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask,\n",
    "            tau=tau, delta=delta\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "\n",
    "        y = x = self.norm1(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm2(x + y), attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None, tau=None, delta=None):\n",
    "        # x [B, L, D]\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for i, (attn_layer, conv_layer) in enumerate(zip(self.attn_layers, self.conv_layers)):\n",
    "                delta = delta if i == 0 else None\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "72ef0b37-3d7e-4e76-b0fe-95d1f10c0ea5",
   "metadata": {},
   "source": [
    "class ProbAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(ProbAttention, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def _prob_QK(self, Q, K, sample_k, n_top):  # n_top: c*ln(L_q)\n",
    "        # Q [B, H, L, D]\n",
    "        B, H, L_K, E = K.shape\n",
    "        _, _, L_Q, _ = Q.shape\n",
    "\n",
    "        # calculate the sampled Q_K\n",
    "        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\n",
    "        # real U = U_part(factor*ln(L_k))*L_q\n",
    "        index_sample = torch.randint(L_K, (L_Q, sample_k))\n",
    "        K_sample = K_expand[:, :, torch.arange(\n",
    "            L_Q).unsqueeze(1), index_sample, :]\n",
    "        Q_K_sample = torch.matmul(\n",
    "            Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze()\n",
    "\n",
    "        # find the Top_k query with sparisty measurement\n",
    "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n",
    "        M_top = M.topk(n_top, sorted=False)[1]\n",
    "\n",
    "        # use the reduced Q to calculate Q_K\n",
    "        Q_reduce = Q[torch.arange(B)[:, None, None],\n",
    "                   torch.arange(H)[None, :, None],\n",
    "                   M_top, :]  # factor*ln(L_q)\n",
    "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))  # factor*ln(L_q)*L_k\n",
    "\n",
    "        return Q_K, M_top\n",
    "\n",
    "    def _get_initial_context(self, V, L_Q):\n",
    "        B, H, L_V, D = V.shape\n",
    "        if not self.mask_flag:\n",
    "            # V_sum = V.sum(dim=-2)\n",
    "            V_sum = V.mean(dim=-2)\n",
    "            contex = V_sum.unsqueeze(-2).expand(B, H,\n",
    "                                                L_Q, V_sum.shape[-1]).clone()\n",
    "        else:  # use mask\n",
    "            # requires that L_Q == L_V, i.e. for self-attention only\n",
    "            assert (L_Q == L_V)\n",
    "            contex = V.cumsum(dim=-2)\n",
    "        return contex\n",
    "\n",
    "    def _update_context(self, context_in, V, scores, index, L_Q, attn_mask):\n",
    "        B, H, L_V, D = V.shape\n",
    "\n",
    "        if self.mask_flag:\n",
    "            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)  # nn.Softmax(dim=-1)(scores)\n",
    "\n",
    "        context_in[torch.arange(B)[:, None, None],\n",
    "        torch.arange(H)[None, :, None],\n",
    "        index, :] = torch.matmul(attn, V).type_as(context_in)\n",
    "        if self.output_attention:\n",
    "            attns = (torch.ones([B, H, L_V, L_V]) /\n",
    "                     L_V).type_as(attn).to(attn.device)\n",
    "            attns[torch.arange(B)[:, None, None], torch.arange(H)[\n",
    "                                                  None, :, None], index, :] = attn\n",
    "            return context_in, attns\n",
    "        else:\n",
    "            return context_in, None\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n",
    "        B, L_Q, H, D = queries.shape\n",
    "        _, L_K, _, _ = keys.shape\n",
    "\n",
    "        queries = queries.transpose(2, 1)\n",
    "        keys = keys.transpose(2, 1)\n",
    "        values = values.transpose(2, 1)\n",
    "\n",
    "        U_part = self.factor * \\\n",
    "                 np.ceil(np.log(L_K)).astype('int').item()  # c*ln(L_k)\n",
    "        u = self.factor * \\\n",
    "            np.ceil(np.log(L_Q)).astype('int').item()  # c*ln(L_q)\n",
    "\n",
    "        U_part = U_part if U_part < L_K else L_K\n",
    "        u = u if u < L_Q else L_Q\n",
    "\n",
    "        scores_top, index = self._prob_QK(\n",
    "            queries, keys, sample_k=U_part, n_top=u)\n",
    "\n",
    "        # add scale factor\n",
    "        scale = self.scale or 1. / sqrt(D)\n",
    "        if scale is not None:\n",
    "            scores_top = scores_top * scale\n",
    "        # get the context\n",
    "        context = self._get_initial_context(values, L_Q)\n",
    "        # update the context with selected top_k queries\n",
    "        context, attn = self._update_context(\n",
    "            context, values, scores_top, index, L_Q, attn_mask)\n",
    "\n",
    "        return context.contiguous(), attn\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask,\n",
    "            tau=tau,\n",
    "            delta=delta\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8b360d22-773a-411d-b1fb-b8db578dfe66",
   "metadata": {},
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Informer with Propspare attention in O(LlogL) complexity\n",
    "    Paper link: https://ojs.aaai.org/index.php/AAAI/article/view/17325/17132\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, enc_in=144, d_model=512, embed='timeF', freq='h', dropout=0.1, factor=1, \n",
    "                 output_attention=False, n_heads=8, d_ff=2048, activation='gelu', e_layers=2, seq_len=200,\n",
    "                 num_class=2):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.enc_embedding = DataEmbedding(enc_in, d_model, embed, freq, dropout)\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        ProbAttention(False, factor, attention_dropout=dropout, output_attention=output_attention),\n",
    "                        d_model, \n",
    "                        n_heads\n",
    "                    ),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            None,\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        self.act = F.gelu\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(d_model * seq_len, num_class)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc):\n",
    "        # enc\n",
    "        enc_out = self.enc_embedding(x_enc, None)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=None)\n",
    "\n",
    "        # Output\n",
    "        output = self.act(enc_out)  # the output transformer encoder/decoder embeddings don't include non-linearity\n",
    "        output = self.dropout(output)\n",
    "        output = output * x_mark_enc.unsqueeze(-1)  # zero-out padding embeddings\n",
    "        output = output.reshape(output.shape[0], -1)  # (batch_size, seq_length * d_model)\n",
    "        output = self.projection(output)  # (batch_size, num_classes)\n",
    "        \n",
    "        return output"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c6e34b66-2bdc-468f-a0af-b57637b69c79",
   "metadata": {},
   "source": [
    "model2 = Model()\n",
    "model2.eval()\n",
    "model2 = model2.to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "41712157-7390-45fa-8fa8-844684bff8a8",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    output2 = model2(X, mask)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "82328555-5b15-43e2-a11a-868d4768c1cf",
   "metadata": {},
   "source": [
    "output2[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6883a9ca-18d9-44df-9838-3daaa5520adb",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8618c-85ed-4e07-a44e-2490bab73092",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a68b52-7d97-46d5-b9c6-e74812a163db",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b0577-4a4d-49a6-8917-469ae2b4c3bd",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d191a5-e6c2-48e7-a6ed-d0a1745753e8",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c31e9-0a11-4a4c-ae1e-7a3a45795027",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a37b79-d690-417b-bd5e-15aa68e2d08d",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0562aa-afbf-40e4-baec-d554278c2037",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a4a450-70ca-4a28-865f-581f4763a03f",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d839cb33-cbf8-4b92-b8bb-41790d64842f",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629b5827-02b2-4895-904a-cd44c9bc6bc6",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
